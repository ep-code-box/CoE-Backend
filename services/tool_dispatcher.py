"""ì‚¬ìš©ìž ìš”ì²­ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ìž¥ ì í•©í•œ ë„êµ¬ë¥¼ ì°¾ì•„ ì‹¤í–‰í•˜ëŠ” ì„œë¹„ìŠ¤ìž…ë‹ˆë‹¤.
ì´ ëª¨ë“ˆì€ ë„êµ¬ ì‹¤í–‰ì˜ ë‘ë‡Œ ì—­í• ì„ í•˜ë©°, Python ë„êµ¬ì™€ LangFlow ì›Œí¬í”Œë¡œìš°ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
import os
import sys
import importlib.util
import json
from typing import Dict, Any, Optional, List
import logging
import httpx

from sqlalchemy.orm import Session
from core.database import LangFlow, SessionLocal, LangflowToolMapping
from core.schemas import AgentState
from core.llm_client import get_client_for_model

logger = logging.getLogger(__name__)

# --- Constants ---
TOOLS_BASE_DIR = "tools"
LANGFLOW_EXECUTION_URL = os.getenv("LANGFLOW_EXECUTION_URL", "http://localhost:8000")
RUN_BEST_LANGFLOW_TOOL_NAME = "run_best_langflow_workflow"

# --- Generic Tool Schema for LangFlow ---
LANGFLOW_GENERIC_SCHEMA = {
    "type": "function",
    "function": {
        "name": RUN_BEST_LANGFLOW_TOOL_NAME,
        "description": "ì‚¬ìš©ìžì˜ ë³µìž¡í•œ ìš”ì²­ì´ë‚˜ ì§ˆë¬¸ì— ê°€ìž¥ ì í•©í•œ ìžë™í™”ëœ ì›Œí¬í”Œë¡œìš°(LangFlow)ë¥¼ ì°¾ì•„ ì‹¤í–‰í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì •ë³´ ì¡°íšŒë‚˜ ê°„ë‹¨í•œ ìž‘ì—…ì´ ì•„ë‹Œ, ì—¬ëŸ¬ ë‹¨ê³„ê°€ í•„ìš”í•´ ë³´ì´ëŠ” ìš”ì²­ì— ì‚¬ìš©ë©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "tool_input": {
                    "type": "object",
                    "description": "ì›Œí¬í”Œë¡œìš°ì— ì „ë‹¬í•  ì¶”ê°€ì ì¸ êµ¬ì¡°í™”ëœ ìž…ë ¥ê°’ìž…ë‹ˆë‹¤."
                }
            }
        }
    }
}

# --- Main Entrypoint ---

async def decide_and_dispatch(state: AgentState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ìž ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ ê²°ì •í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    context = state.get("context")
    model_id = state.get("model_id")
    history = state.get("history", [])

    # 1. Get all available tools for the current context
    tool_schemas = get_available_tools_for_context(context)

    if not tool_schemas:
        logger.warning(f"No tools available for context '{context}'. Returning no-op.")
        history.append({"role": "assistant", "content": "í˜„ìž¬ ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤."})
        return {"history": history}

    logger.info(f"Calling LLM to decide action for context '{context}'.")
    llm_client = get_client_for_model(model_id)

    # Construct tool descriptions string for the prompt
    tool_descriptions_string = "\n".join(
        [f"- '{tool['function']['name']}': {tool['function']['description']}" for tool in tool_schemas]
    )

    system_prompt_content = f"""ë‹¹ì‹ ì€ ì§€ëŠ¥í˜• ë””ìŠ¤íŒ¨ì²˜ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬, ì£¼ì–´ì§„ ìž‘ì—…ì„ ì™„ìˆ˜í•˜ê¸° ìœ„í•´ ê°€ìž¥ ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.
ì œê³µëœ ë„êµ¬ ëª©ë¡ê³¼ ê° ë„êµ¬ì˜ ì„¤ëª…ì„ ì‹ ì¤‘í•˜ê²Œ ê²€í† í•˜ì—¬ ìµœì ì˜ ê²°ì •ì„ ë‚´ë¦¬ì„¸ìš”.

--- ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ---
{tool_descriptions_string}

--- ì‘ë‹µ í˜•ì‹ ---
ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤: {{'next_tool': 'ì„ íƒí•œ ë„êµ¬ ì´ë¦„'}}
ë§Œì•½ ì‚¬ìš©ìžì˜ ìš”ì²­ì„ ì²˜ë¦¬í•  ì í•©í•œ ë„êµ¬ê°€ ì—†ë‹¤ë©´, "none"ì„ ì„ íƒí•˜ì„¸ìš”.
"""
    system_prompt = {
        "role": "system",
        "content": system_prompt_content
    }
    messages_for_llm = history + [system_prompt]

    response = llm_client.chat.completions.create(
        model=model_id,
        messages=messages_for_llm,
        response_format={"type": "json_object"}, # Force JSON output
        temperature=0 # ë„êµ¬ ì„ íƒì˜ ê²°ì •ë¡ ì  íŒë‹¨ì„ ìœ„í•´ temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì •
    )
    response_message = response.choices[0].message.model_dump()
    history.append(response_message) # Append the LLM's response to history

    # Parse the LLM's JSON response
    try:
        choice_json = json.loads(response_message["content"])
        chosen_tool_name = choice_json.get("next_tool")
        logger.info(f"DEBUG: Parsed chosen_tool_name: '{chosen_tool_name}' (type: {type(chosen_tool_name)})") # ì´ ì¤„ ì¶”ê°€
    except (json.JSONDecodeError, KeyError) as e:
        error_msg = f"LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}. ì‘ë‹µ ë‚´ìš©: {response_message.get('content', 'N/A')}"
        logger.error(f"ðŸš¨ [TOOL_SELECTION_ERROR] {error_msg}")
        history.append({"role": "system", "content": error_msg})
        return {"history": history}

    logger.info(f"LLM chose to execute tool: {chosen_tool_name}")

    # Check if a tool was chosen or if LLM indicated no tool
    if chosen_tool_name == "none":
        logger.info("LLM decided not to use any tool (explicitly 'none').")
        # ë„êµ¬ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ìžì—°ì–´ ì‘ë‹µì„ ìœ„í•´ 'ì†Œí†µê°€' LLMì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
        pass # ì•„ëž˜ 'ì†Œí†µê°€' LLM í˜¸ì¶œ ë¶€ë¶„ìœ¼ë¡œ íë¦„ì„ ë„˜ê¹ë‹ˆë‹¤.

    # Validate chosen tool name against available tool schemas
    valid_tool_names = {tool['function']['name'] for tool in tool_schemas}
    if chosen_tool_name not in valid_tool_names:
        error_msg = f"LLMì´ ìœ íš¨í•˜ì§€ ì•Šì€ ë„êµ¬({chosen_tool_name})ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ ë„êµ¬: {', '.join(valid_tool_names)}"
        logger.error(f"ðŸš¨ [TOOL_SELECTION_ERROR] {error_msg}")
        history.append({"role": "system", "content": error_msg})
        return {"history": history}

    # Find the actual tool schema for the chosen tool to get its parameters
    chosen_tool_schema = next((tool for tool in tool_schemas if tool['function']['name'] == chosen_tool_name), None)
    if not chosen_tool_schema:
        error_msg = f"ì„ íƒëœ ë„êµ¬({chosen_tool_name})ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        logger.error(f"ðŸš¨ [TOOL_SELECTION_ERROR] {error_msg}")
        history.append({"role": "system", "content": error_msg})
        return {"history": history}

    # This is a simplified approach for passing arguments.
    # A more robust solution would involve the LLM extracting arguments from the user's query.
    last_user_message_content = ""
    for msg in reversed(history):
        if msg.get("role") == "user":
            last_user_message_content = msg.get("content", "")
            break

    tool_args = {"input": last_user_message_content}

    # 4. Dispatch to the chosen tool executor
    result = {}
    if chosen_tool_name == RUN_BEST_LANGFLOW_TOOL_NAME:
        result = await find_and_run_best_flow(state, tool_args)
    else:
        python_tool_path = find_python_tool_path(chosen_tool_name, context)
        if python_tool_path:
            result = await run_python_tool(python_tool_path, tool_args, state)
        else:
            result = {"error": f"Tool '{chosen_tool_name}' was selected by LLM but not found in dispatcher."}
    
    # --- ì¶”ê°€ë  ë¶€ë¶„: visualize_conversation_as_langflow ë„êµ¬ì¼ ê²½ìš° ë°”ë¡œ JSON ë°˜í™˜ ---
    if chosen_tool_name == "visualize_conversation_as_langflow":
        # ë„êµ¬ì˜ ê²°ê³¼(result)ê°€ ì´ë¯¸ LangFlow JSON ë¬¸ìžì—´ì´ë¯€ë¡œ, ì´ë¥¼ ë°”ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        # OpenAI í˜¸í™˜ ì‘ë‹µ í˜•ì‹ì— ë§žì¶° assistant ë©”ì‹œì§€ë¡œ ëž˜í•‘í•©ë‹ˆë‹¤.
        return {"history": [{"role": "assistant", "content": result}]}
    # --- ì¶”ê°€ë  ë¶€ë¶„ ë ---

    # 5. Append tool result to history (ì´ ë¶€ë¶„ì€ ì´ì œ visualize_conversation_as_langflow ë„êµ¬ì¼ ê²½ìš° ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
    history.append({
        "role": "system",
        "content": f"Tool '{chosen_tool_name}' was executed and returned the following result:\n\n{json.dumps(result, ensure_ascii=False, indent=2)}",
    })

    # 6. Call LLM again to get a natural language response (ì´ ë¶€ë¶„ë„ visualize_conversation_as_langflow ë„êµ¬ì¼ ê²½ìš° ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
    logger.info("Calling LLM again to synthesize final response from tool result.")
    second_response = llm_client.chat.completions.create(
        model=model_id,
        messages=history,
    )
    history.append(second_response.choices[0].message.model_dump())

    return {"history": history}


# --- Tool Discovery & Execution ---

def get_available_tools_for_context(context: str) -> List[Dict[str, Any]]:
    """
    ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë„êµ¬ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    all_schemas: List[Dict[str, Any]] = []

    if not context:
        logger.warning("Context not provided, cannot determine available tools.")
        return all_schemas
        
    # 1. Scan for Python tools
    tools_dir = os.path.abspath(TOOLS_BASE_DIR)
    for root, _, files in os.walk(tools_dir):
        for filename in files:
            if filename.endswith("_map.py"):
                map_filepath = os.path.join(root, filename)
                try:
                    spec = importlib.util.spec_from_file_location("map_module", map_filepath)
                    map_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(map_module)
                    
                    if context in getattr(map_module, 'tool_contexts', []):
                        tool_filepath = map_filepath.replace("_map.py", "_tool.py")
                        if os.path.exists(tool_filepath):
                            tool_spec = importlib.util.spec_from_file_location("tool_module", tool_filepath)
                            tool_module = importlib.util.module_from_spec(tool_spec)
                            tool_spec.loader.exec_module(tool_module)
                            if hasattr(tool_module, 'available_tools'):
                                all_schemas.extend(getattr(tool_module, 'available_tools'))
                except Exception as e:
                    logger.error(f"Error loading tools from map file {map_filepath}: {e}")

    # 2. Check for LangFlows and add generic executor tool
    db: Session = SessionLocal()
    try:
        if db.query(LangFlow).join(LangflowToolMapping).filter(LangflowToolMapping.tool_contexts == context, LangFlow.is_active == True).first():
            all_schemas.append(LANGFLOW_GENERIC_SCHEMA)
    except Exception as e:
        logger.error(f"Error checking for LangFlows in context '{context}': {e}")
    finally:
        db.close()

    return all_schemas

def find_python_tool_path(tool_name: str, context: Optional[str]) -> Optional[str]:
    """
    Finds the file path of a Python tool based on its name and context.
    """
    if not context:
        return None
    tools_dir = os.path.abspath(TOOLS_BASE_DIR)
    for root, _, files in os.walk(tools_dir):
        for filename in files:
            if filename.endswith("_map.py"):
                map_filepath = os.path.join(root, filename)
                try:
                    spec = importlib.util.spec_from_file_location("map_module", map_filepath)
                    map_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(map_module)
                    if context in getattr(map_module, 'tool_contexts', []):
                        tool_filepath = map_filepath.replace("_map.py", "_tool.py")
                        if os.path.exists(tool_filepath):
                            tool_spec = importlib.util.spec_from_file_location("tool_module", tool_filepath)
                            tool_module = importlib.util.module_from_spec(tool_spec)
                            tool_spec.loader.exec_module(tool_module)
                            if tool_name in getattr(tool_module, 'tool_functions', {}):
                                return tool_filepath
                except Exception as e:
                    logger.error(f"Error processing map file {map_filepath}: {e}")
    return None

async def run_python_tool(tool_path: str, tool_input: Optional[Dict[str, Any]], state: AgentState) -> Any:
    """
    Dynamically loads and executes a Python tool from a given file path.
    """
    try:
        module_name = os.path.splitext(os.path.relpath(tool_path, "."))[0].replace(os.path.sep, '.')
        spec = importlib.util.spec_from_file_location(module_name, tool_path)
        tool_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(tool_module)
        
        # The entrypoint for all python tools is the 'run' function.
        if hasattr(tool_module, 'run') and callable(getattr(tool_module, 'run')):
            return await getattr(tool_module, 'run')(tool_input, state)
        else:
            raise AttributeError(f"Tool module at {tool_path} does not have a callable 'run' function.")
    except Exception as e:
        logger.error(f"Failed to execute Python tool at {tool_path}: {e}", exc_info=True)
        return {"error": f"An error occurred while running the tool: {str(e)}"}

async def find_and_run_best_flow(state: AgentState, tool_input: Optional[Dict[str, Any]]) -> Any:
    """
    Uses an LLM to find the best flow based on its description and executes it.
    """
    user_query = state.get("input")
    context = state.get("context")
    model_id = state.get("model_id")

    if not user_query or not context:
        return {"error": "User query or context missing for semantic flow search."}

    db: Session = SessionLocal()
    try:
        available_flows = db.query(LangFlow).filter(LangFlow.context == context, LangFlow.is_active == True).all()
        if not available_flows:
            return {"message": "í•´ë‹¹ ì»¨í…ìŠ¤íŠ¸ì— ì‹¤í–‰ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        flow_descriptions = "\n".join([f"- {flow.name}: {flow.description}" for flow in available_flows])
        prompt = f"""Based on the user's query, which of the following workflows is the best to run?
User Query: "{user_query}"

Available Workflows:
{flow_descriptions}

Respond with the exact name of the best workflow and nothing else."""

        logger.info(f"Calling LLM to choose the best LangFlow from {len(available_flows)} options.")
        llm_client = get_client_for_model(model_id)
        response = llm_client.chat.completions.create(
            model=model_id,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        chosen_flow_name = response.choices[0].message.content.strip()

        chosen_flow = next((flow for flow in available_flows if flow.name == chosen_flow_name), None)

        if not chosen_flow:
            logger.error(f"LLM chose a non-existent flow: '{chosen_flow_name}'")
            return {"error": f"LLM chose a non-existent workflow: {chosen_flow_name}"}

        logger.info(f"Executing best-matched LangFlow: {chosen_flow.name}")
        return await run_langflow_tool(chosen_flow, tool_input, state)

    except Exception as e:
        logger.error(f"Error during semantic search and execution of LangFlow: {e}", exc_info=True)
        return {"error": f"An unexpected error occurred: {str(e)}"}
    finally:
        db.close()

async def run_langflow_tool(flow: LangFlow, tool_input: Optional[Dict[str, Any]], state: AgentState) -> Any:
    """
    Executes a given LangFlow workflow.
    """
    execution_url = f"{LANGFLOW_EXECUTION_URL}/flows/run/{flow.name}"
    # The user's direct input might be in the tool_input from the LLM or the original state input
    final_tool_input = tool_input or {"input": state.get("input")}
    request_body = {"user_input": final_tool_input}

    logger.info(f"Calling LangFlow execution endpoint: {execution_url}")
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(execution_url, json=request_body, timeout=60.0)
            response.raise_for_status()
            return response.json()
    except httpx.RequestError as e:
        logger.error(f"HTTP request to LangFlow failed: {e}", exc_info=True)
        return {"error": f"Failed to connect to LangFlow service: {str(e)}"}
    except httpx.HTTPStatusError as e:
        logger.error(f"LangFlow service returned an error: {e.response.status_code} {e.response.text}", exc_info=True)
        return {"error": f"LangFlow service error: {e.response.status_code}", "detail": e.response.text}
    except Exception as e:
        logger.error(f"Failed to execute LangFlow tool '{flow.name}': {e}", exc_info=True)
        return {"error": f"An unexpected error occurred while running the LangFlow tool: {str(e)}"}