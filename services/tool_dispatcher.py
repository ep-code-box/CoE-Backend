"""ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠Ïùò ÏùòÎèÑÎ•º Î∂ÑÏÑùÌïòÏó¨ Í∞ÄÏû• Ï†ÅÌï©Ìïú ÎèÑÍµ¨Î•º Ï∞æÏïÑ Ïã§ÌñâÌïòÎäî ÏÑúÎπÑÏä§ÏûÖÎãàÎã§.
Ïù¥ Î™®ÎìàÏùÄ ÎèÑÍµ¨ Ïã§ÌñâÏùò ÎëêÎáå Ïó≠Ìï†ÏùÑ ÌïòÎ©∞, Python ÎèÑÍµ¨ÏôÄ LangFlow ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º Î™®Îëê Ï≤òÎ¶¨Ìï©ÎãàÎã§."""
import os
import sys
import importlib.util
import json
from typing import Dict, Any, Optional, List
import logging
import httpx

from sqlalchemy.orm import Session
from core.database import LangFlow, SessionLocal, LangflowToolMapping
from core.schemas import AgentState
from core.llm_client import get_client_for_model

logger = logging.getLogger(__name__)

# --- Constants ---
TOOLS_BASE_DIR = "tools"
LANGFLOW_EXECUTION_URL = os.getenv("LANGFLOW_EXECUTION_URL", "http://localhost:8000")
RUN_BEST_LANGFLOW_TOOL_NAME = "run_best_langflow_workflow"

# --- Generic Tool Schema for LangFlow ---
LANGFLOW_GENERIC_SCHEMA = {
    "type": "function",
    "function": {
        "name": RUN_BEST_LANGFLOW_TOOL_NAME,
        "description": "ÏÇ¨Ïö©ÏûêÏùò Î≥µÏû°Ìïú ÏöîÏ≤≠Ïù¥ÎÇò ÏßàÎ¨∏Ïóê Í∞ÄÏû• Ï†ÅÌï©Ìïú ÏûêÎèôÌôîÎêú ÏõåÌÅ¨ÌîåÎ°úÏö∞(LangFlow)Î•º Ï∞æÏïÑ Ïã§ÌñâÌï©ÎãàÎã§. ÏùºÎ∞òÏ†ÅÏù∏ Ï†ïÎ≥¥ Ï°∞ÌöåÎÇò Í∞ÑÎã®Ìïú ÏûëÏóÖÏù¥ ÏïÑÎãå, Ïó¨Îü¨ Îã®Í≥ÑÍ∞Ä ÌïÑÏöîÌï¥ Î≥¥Ïù¥Îäî ÏöîÏ≤≠Ïóê ÏÇ¨Ïö©Îê©ÎãàÎã§.",
        "parameters": {
            "type": "object",
            "properties": {
                "tool_input": {
                    "type": "object",
                    "description": "ÏõåÌÅ¨ÌîåÎ°úÏö∞Ïóê Ï†ÑÎã¨Ìï† Ï∂îÍ∞ÄÏ†ÅÏù∏ Íµ¨Ï°∞ÌôîÎêú ÏûÖÎ†•Í∞íÏûÖÎãàÎã§."
                }
            }
        }
    }
}

# --- Main Entrypoint ---

async def decide_and_dispatch(state: AgentState) -> Dict[str, Any]:
    """
    ÏÇ¨Ïö©Ïûê ÏùòÎèÑÎ•º Î∂ÑÏÑùÌïòÏó¨ Ï†ÅÏ†àÌïú ÎèÑÍµ¨Î•º Í≤∞Ï†ïÌïòÍ≥† Ïã§ÌñâÌï©ÎãàÎã§.
    """
    context = state.get("context")
    model_id = state.get("model_id")
    history = state.get("history", [])

    # 1. Get all available tools for the current context
    tool_schemas = get_available_tools_for_context(context)

    if not tool_schemas:
        logger.warning(f"No tools available for context '{context}'. Returning no-op.")
        history.append({"role": "assistant", "content": "ÌòÑÏû¨ Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑúÎäî ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî ÎèÑÍµ¨Í∞Ä ÏóÜÏäµÎãàÎã§."})
        return {"history": history}

    logger.info(f"Calling LLM to decide action for context '{context}'.")
    llm_client = get_client_for_model(model_id)

    # Construct tool descriptions string for the prompt
    tool_descriptions_string = "\n".join(
        [f"- '{tool['function']['name']}': {tool['function']['description']}" for tool in tool_schemas]
    )

    system_prompt_content = f"""ÎãπÏã†ÏùÄ ÏßÄÎä•Ìòï ÎîîÏä§Ìå®Ï≤òÏûÖÎãàÎã§. ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨, Ï£ºÏñ¥ÏßÑ ÏûëÏóÖÏùÑ ÏôÑÏàòÌïòÍ∏∞ ÏúÑÌï¥ Í∞ÄÏû• Ï†ÅÌï©Ìïú ÎèÑÍµ¨Î•º ÏÑ†ÌÉùÌï¥Ïïº Ìï©ÎãàÎã§.
Ï†úÍ≥µÎêú ÎèÑÍµ¨ Î™©Î°ùÍ≥º Í∞Å ÎèÑÍµ¨Ïùò ÏÑ§Î™ÖÏùÑ Ïã†Ï§ëÌïòÍ≤å Í≤ÄÌÜ†ÌïòÏó¨ ÏµúÏ†ÅÏùò Í≤∞Ï†ïÏùÑ ÎÇ¥Î¶¨ÏÑ∏Ïöî.

--- ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨ ---
{tool_descriptions_string}

--- ÏùëÎãµ ÌòïÏãù ---
ÏùëÎãµÏùÄ Î∞òÎìúÏãú Îã§Ïùå JSON ÌòïÏãùÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§: {{'next_tool': 'ÏÑ†ÌÉùÌïú ÎèÑÍµ¨ Ïù¥Î¶Ñ'}}
ÎßåÏïΩ ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠ÏùÑ Ï≤òÎ¶¨Ìï† Ï†ÅÌï©Ìïú ÎèÑÍµ¨Í∞Ä ÏóÜÎã§Î©¥, "none"ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî.
"""
    system_prompt = {
        "role": "system",
        "content": system_prompt_content
    }
    messages_for_llm = history + [system_prompt]

    response = llm_client.chat.completions.create(
        model=model_id,
        messages=messages_for_llm,
        response_format={"type": "json_object"}, # Force JSON output
        temperature=0 # ÎèÑÍµ¨ ÏÑ†ÌÉùÏùò Í≤∞Ï†ïÎ°†Ï†Å ÌåêÎã®ÏùÑ ÏúÑÌï¥ temperatureÎ•º 0ÏúºÎ°ú ÏÑ§Ï†ï
    )
    response_message = response.choices[0].message.model_dump()
    history.append(response_message) # Append the LLM's response to history

    # Parse the LLM's JSON response
    try:
        choice_json = json.loads(response_message["content"])
        chosen_tool_name = choice_json.get("next_tool")
        logger.info(f"DEBUG: Parsed chosen_tool_name: '{chosen_tool_name}' (type: {type(chosen_tool_name)})") # Ïù¥ Ï§Ñ Ï∂îÍ∞Ä
    except (json.JSONDecodeError, KeyError) as e:
        error_msg = f"LLM ÏùëÎãµ ÌååÏã± Ïã§Ìå®: {e}. ÏùëÎãµ ÎÇ¥Ïö©: {response_message.get('content', 'N/A')}"
        logger.error(f"üö® [TOOL_SELECTION_ERROR] {error_msg}")
        history.append({"role": "system", "content": error_msg})
        return {"history": history}

    logger.info(f"LLM chose to execute tool: {chosen_tool_name}")

    # Check if a tool was chosen or if LLM indicated no tool
    if chosen_tool_name == "none":
        logger.info("LLM decided not to use any tool (explicitly 'none').")
        # ÎèÑÍµ¨Í∞Ä Ïã§ÌñâÎêòÏßÄ ÏïäÏïòÏßÄÎßå, ÏûêÏó∞Ïñ¥ ÏùëÎãµÏùÑ ÏúÑÌï¥ 'ÏÜåÌÜµÍ∞Ä' LLMÏùÑ Ìò∏Ï∂úÌï¥Ïïº Ìï©ÎãàÎã§.
        pass # ÏïÑÎûò 'ÏÜåÌÜµÍ∞Ä' LLM Ìò∏Ï∂ú Î∂ÄÎ∂ÑÏúºÎ°ú ÌùêÎ¶ÑÏùÑ ÎÑòÍπÅÎãàÎã§.

    # Validate chosen tool name against available tool schemas
    valid_tool_names = {tool['function']['name'] for tool in tool_schemas}
    if chosen_tool_name not in valid_tool_names:
        error_msg = f"LLMÏù¥ Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ ÎèÑÍµ¨({chosen_tool_name})Î•º Î∞òÌôòÌñàÏäµÎãàÎã§. Ïú†Ìö®Ìïú ÎèÑÍµ¨: {', '.join(valid_tool_names)}"
        logger.error(f"üö® [TOOL_SELECTION_ERROR] {error_msg}")
        history.append({"role": "system", "content": error_msg})
        return {"history": history}

    # Find the actual tool schema for the chosen tool to get its parameters
    chosen_tool_schema = next((tool for tool in tool_schemas if tool['function']['name'] == chosen_tool_name), None)
    if not chosen_tool_schema:
        error_msg = f"ÏÑ†ÌÉùÎêú ÎèÑÍµ¨({chosen_tool_name})Ïùò Ïä§ÌÇ§ÎßàÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
        logger.error(f"üö® [TOOL_SELECTION_ERROR] {error_msg}")
        history.append({"role": "system", "content": error_msg})
        return {"history": history}

    # This is a simplified approach for passing arguments.
    # A more robust solution would involve the LLM extracting arguments from the user's query.
    last_user_message_content = ""
    for msg in reversed(history):
        if msg.get("role") == "user":
            last_user_message_content = msg.get("content", "")
            break

    tool_args = {"input": last_user_message_content}

    # 4. Dispatch to the chosen tool executor
    result = {}
    if chosen_tool_name == RUN_BEST_LANGFLOW_TOOL_NAME:
        result = await find_and_run_best_flow(state, tool_args)
    else:
        python_tool_path = find_python_tool_path(chosen_tool_name, context)
        if python_tool_path:
            result = await run_python_tool(python_tool_path, tool_args, state)
        else:
            result = {"error": f"Tool '{chosen_tool_name}' was selected by LLM but not found in dispatcher."}
    
    # --- Ï∂îÍ∞ÄÎê† Î∂ÄÎ∂Ñ: visualize_conversation_as_langflow ÎèÑÍµ¨Ïùº Í≤ΩÏö∞ Î∞îÎ°ú JSON Î∞òÌôò ---
    if chosen_tool_name == "visualize_conversation_as_langflow":
        # ÎèÑÍµ¨Ïùò Í≤∞Í≥º(result)Í∞Ä Ïù¥ÎØ∏ LangFlow JSON Î¨∏ÏûêÏó¥Ïù¥ÎØÄÎ°ú, Ïù¥Î•º Î∞îÎ°ú Î∞òÌôòÌï©ÎãàÎã§.
        # OpenAI Ìò∏Ìôò ÏùëÎãµ ÌòïÏãùÏóê ÎßûÏ∂∞ assistant Î©îÏãúÏßÄÎ°ú ÎûòÌïëÌï©ÎãàÎã§.
        return {"history": [{"role": "assistant", "content": result}]}
    # --- Ï∂îÍ∞ÄÎê† Î∂ÄÎ∂Ñ ÎÅù ---

    # 5. Append tool result to history (Ïù¥ Î∂ÄÎ∂ÑÏùÄ Ïù¥Ï†ú visualize_conversation_as_langflow ÎèÑÍµ¨Ïùº Í≤ΩÏö∞ Ïã§ÌñâÎêòÏßÄ ÏïäÏùå)
    history.append({
        "role": "system",
        "content": f"Tool '{chosen_tool_name}' was executed and returned the following result:\n\n{json.dumps(result, ensure_ascii=False, indent=2)}",
    })

    # 6. Call LLM again to get a natural language response (Ïù¥ Î∂ÄÎ∂ÑÎèÑ visualize_conversation_as_langflow ÎèÑÍµ¨Ïùº Í≤ΩÏö∞ Ïã§ÌñâÎêòÏßÄ ÏïäÏùå)
    logger.info("Calling LLM again to synthesize final response from tool result.")
    second_response = llm_client.chat.completions.create(
        model=model_id,
        messages=history,
    )
    history.append(second_response.choices[0].message.model_dump())

    return {"history": history}


# --- Tool Discovery & Execution ---

def get_available_tools_for_context(context: str) -> List[Dict[str, Any]]:
    """
    Ï£ºÏñ¥ÏßÑ Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îì† ÎèÑÍµ¨Ïùò Ïä§ÌÇ§ÎßàÎ•º Î∞òÌôòÌï©ÎãàÎã§.
    """
    all_schemas: List[Dict[str, Any]] = []

    if not context:
        logger.warning("Context not provided, cannot determine available tools.")
        return all_schemas
        
    # 1. Scan for Python tools
    tools_dir = os.path.abspath(TOOLS_BASE_DIR)
    for root, _, files in os.walk(tools_dir):
        for filename in files:
            if filename.endswith("_map.py"):
                map_filepath = os.path.join(root, filename)
                try:
                    spec = importlib.util.spec_from_file_location("map_module", map_filepath)
                    map_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(map_module)
                    
                    if context in getattr(map_module, 'tool_contexts', []):
                        tool_filepath = map_filepath.replace("_map.py", "_tool.py")
                        if os.path.exists(tool_filepath):
                            tool_spec = importlib.util.spec_from_file_location("tool_module", tool_filepath)
                            tool_module = importlib.util.module_from_spec(tool_spec)
                            tool_spec.loader.exec_module(tool_module)
                            if hasattr(tool_module, 'available_tools'):
                                all_schemas.extend(getattr(tool_module, 'available_tools'))
                except Exception as e:
                    logger.error(f"Error loading tools from map file {map_filepath}: {e}")

    # 2. Check for LangFlows and add generic executor tool
    db: Session = SessionLocal()
    try:
        if db.query(LangFlow).join(LangflowToolMapping).filter(LangflowToolMapping.tool_contexts == context, LangFlow.is_active == True).first():
            all_schemas.append(LANGFLOW_GENERIC_SCHEMA)
    except Exception as e:
        logger.error(f"Error checking for LangFlows in context '{context}': {e}")
    finally:
        db.close()

    return all_schemas

def find_python_tool_path(tool_name: str, context: Optional[str]) -> Optional[str]:
    """
    Finds the file path of a Python tool based on its name and context.
    """
    if not context:
        return None
    tools_dir = os.path.abspath(TOOLS_BASE_DIR)
    for root, _, files in os.walk(tools_dir):
        for filename in files:
            if filename.endswith("_map.py"):
                map_filepath = os.path.join(root, filename)
                try:
                    spec = importlib.util.spec_from_file_location("map_module", map_filepath)
                    map_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(map_module)
                    if context in getattr(map_module, 'tool_contexts', []):
                        tool_filepath = map_filepath.replace("_map.py", "_tool.py")
                        if os.path.exists(tool_filepath):
                            tool_spec = importlib.util.spec_from_file_location("tool_module", tool_filepath)
                            tool_module = importlib.util.module_from_spec(tool_spec)
                            tool_spec.loader.exec_module(tool_module)
                            if tool_name in getattr(tool_module, 'tool_functions', {}):
                                return tool_filepath
                except Exception as e:
                    logger.error(f"Error processing map file {map_filepath}: {e}")
    return None

async def run_python_tool(tool_path: str, tool_input: Optional[Dict[str, Any]], state: AgentState) -> Any:
    """
    Dynamically loads and executes a Python tool from a given file path.
    """
    try:
        module_name = os.path.splitext(os.path.relpath(tool_path, "."))[0].replace(os.path.sep, '.')
        spec = importlib.util.spec_from_file_location(module_name, tool_path)
        tool_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(tool_module)
        
        # The entrypoint for all python tools is the 'run' function.
        if hasattr(tool_module, 'run') and callable(getattr(tool_module, 'run')):
            return await getattr(tool_module, 'run')(tool_input, state)
        else:
            raise AttributeError(f"Tool module at {tool_path} does not have a callable 'run' function.")
    except Exception as e:
        logger.error(f"Failed to execute Python tool at {tool_path}: {e}", exc_info=True)
        return {"error": f"An error occurred while running the tool: {str(e)}"}

async def find_and_run_best_flow(state: AgentState, tool_input: Optional[Dict[str, Any]]) -> Any:
    """
    Uses an LLM to find the best flow based on its description and executes it.
    """
    user_query = state.get("input")
    context = state.get("context")
    model_id = state.get("model_id")

    if not user_query or not context:
        return {"error": "User query or context missing for semantic flow search."}

    db: Session = SessionLocal()
    try:
        available_flows = db.query(LangFlow).filter(LangFlow.context == context, LangFlow.is_active == True).all()
        if not available_flows:
            return {"message": "Ìï¥Îãπ Ïª®ÌÖçÏä§Ìä∏Ïóê Ïã§Ìñâ Í∞ÄÎä•Ìïú ÏõåÌÅ¨ÌîåÎ°úÏö∞Í∞Ä ÏóÜÏäµÎãàÎã§."}

        flow_descriptions = "\n".join([f"- {flow.name}: {flow.description}" for flow in available_flows])
        prompt = f"""Based on the user's query, which of the following workflows is the best to run?
User Query: "{user_query}"

Available Workflows:
{flow_descriptions}

Respond with the exact name of the best workflow and nothing else."""

        logger.info(f"Calling LLM to choose the best LangFlow from {len(available_flows)} options.")
        llm_client = get_client_for_model(model_id)
        response = llm_client.chat.completions.create(
            model=model_id,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        chosen_flow_name = response.choices[0].message.content.strip()

        chosen_flow = next((flow for flow in available_flows if flow.name == chosen_flow_name), None)

        if not chosen_flow:
            logger.error(f"LLM chose a non-existent flow: '{chosen_flow_name}'")
            return {"error": f"LLM chose a non-existent workflow: {chosen_flow_name}"}

        logger.info(f"Executing best-matched LangFlow: {chosen_flow.name}")
        return await run_langflow_tool(chosen_flow, tool_input, state)

    except Exception as e:
        logger.error(f"Error during semantic search and execution of LangFlow: {e}", exc_info=True)
        return {"error": f"An unexpected error occurred: {str(e)}"}
    finally:
        db.close()

async def run_langflow_tool(flow: LangFlow, tool_input: Optional[Dict[str, Any]], state: AgentState) -> Any:
    """
    Executes a given LangFlow workflow.
    """
    execution_url = f"{LANGFLOW_EXECUTION_URL}/flows/run/{flow.name}"
    # The user's direct input might be in the tool_input from the LLM or the original state input
    final_tool_input = tool_input or {"input": state.get("input")}
    request_body = {"user_input": final_tool_input}

    logger.info(f"Calling LangFlow execution endpoint: {execution_url}")
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(execution_url, json=request_body, timeout=60.0)
            response.raise_for_status()
            return response.json()
    except httpx.RequestError as e:
        logger.error(f"HTTP request to LangFlow failed: {e}", exc_info=True)
        return {"error": f"Failed to connect to LangFlow service: {str(e)}"}
    except httpx.HTTPStatusError as e:
        logger.error(f"LangFlow service returned an error: {e.response.status_code} {e.response.text}", exc_info=True)
        return {"error": f"LangFlow service error: {e.response.status_code}", "detail": e.response.text}
    except Exception as e:
        logger.error(f"Failed to execute LangFlow tool '{flow.name}': {e}", exc_info=True)
        return {"error": f"An unexpected error occurred while running the LangFlow tool: {str(e)}"}