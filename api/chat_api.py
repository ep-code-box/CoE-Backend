"""
ì±„íŒ… ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

import time
import uuid
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from core.schemas import ChatRequest, ChatResponse, OpenAIChatRequest
from core.llm_client import client
from utils.streaming_utils import agent_stream_generator, proxy_stream_generator
from tools.utils import find_last_user_message

# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì •ë³´ ì €ì¥
_agent = None
_agent_model_id = None

router = APIRouter(
    tags=["ğŸ¤– AI Chat"],
    prefix="/v1",
    responses={
        200: {"description": "ì±„íŒ… ì‘ë‹µ ì„±ê³µ"},
        400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"description": "ì„œë²„ ì˜¤ë¥˜"}
    }
)


async def handle_agent_request(req: OpenAIChatRequest, agent, agent_model_id: str):
    """
    CoE ì—ì´ì „íŠ¸ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
        
    Returns:
        ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ JSON ì‘ë‹µ
    """
    state = {"messages": [msg.model_dump() for msg in req.messages]}
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    result = await agent.ainvoke(state)
    final_message = find_last_user_message(result["messages"], role="assistant")

    if req.stream:
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        return StreamingResponse(
            agent_stream_generator(req.model, final_message),
            media_type="text/event-stream"
        )
    else:
        # ì¼ë°˜ JSON ì‘ë‹µ
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": req.model,
            "choices": [{"index": 0, "message": {"role": "assistant", "content": final_message}, "finish_reason": "stop"}],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}  # ì‚¬ìš©ëŸ‰ì€ ì¶”ì í•˜ì§€ ì•ŠìŒ
        }


async def handle_llm_proxy_request(req: OpenAIChatRequest):
    """
    ì¼ë°˜ LLM ëª¨ë¸ ìš”ì²­ì„ í”„ë¡ì‹œí•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        
    Returns:
        í”„ë¡ì‹œëœ LLM ì‘ë‹µ
    """
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ ìš”ì²­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
        response = client.chat.completions.create(
            model=req.model,
            messages=[msg.model_dump() for msg in req.messages],
            stream=req.stream,
            temperature=req.temperature,
            max_tokens=req.max_tokens
        )
        
        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í”„ë¡ì‹œ
            return StreamingResponse(
                proxy_stream_generator(response), 
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ í”„ë¡ì‹œ
            return response.model_dump()

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")


def set_agent_info(agent, agent_model_id: str):
    """
    ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ì „ì—­ ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
    """
    global _agent, _agent_model_id
    _agent = agent
    _agent_model_id = agent_model_id


@router.post(
    "/chat/completions",
    summary="AI ì±„íŒ… ì™„ì„±",
    description="""
    **OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.**
    
    ìš”ì²­ëœ ëª¨ë¸ IDì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤:
    - **CoE ì—ì´ì „íŠ¸ ëª¨ë¸**: LangGraph ê¸°ë°˜ AI ì—ì´ì „íŠ¸ ì‹¤í–‰
    - **ì¼ë°˜ LLM ëª¨ë¸**: OpenAI/Anthropic ë“± ì™¸ë¶€ LLM API í”„ë¡ì‹œ
    
    ### ğŸ¤– ì§€ì› ëª¨ë¸
    - `coe-agent-v1`: CoE LangGraph ì—ì´ì „íŠ¸ (ì¶”ì²œ)
    - `gpt-4`, `gpt-3.5-turbo`: OpenAI ëª¨ë¸
    - `claude-3-sonnet`: Anthropic ëª¨ë¸
    
    ### ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    ```bash
    curl -X POST "http://localhost:8000/v1/chat/completions" \\
      -H "Content-Type: application/json" \\
      -d '{
        "model": "coe-agent-v1",
        "messages": [
          {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! CoE ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤."}
        ],
        "stream": false
      }'
    ```
    
    ### ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    `"stream": true` ì„¤ì •ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """,
    response_description="ì±„íŒ… ì™„ì„± ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” JSON)"
)
async def chat_completions(req: OpenAIChatRequest):
    """OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ - CoE ì—ì´ì „íŠ¸ ë˜ëŠ” ì™¸ë¶€ LLM í˜¸ì¶œ"""
    print(f"DEBUG: Received request: {req}")
    print(f"DEBUG: Agent model ID: {_agent_model_id}")
    
    # 1. CoE ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš°
    if req.model == _agent_model_id:
        return await handle_agent_request(req, _agent, _agent_model_id)
    # 2. ì¼ë°˜ LLM ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš° (í”„ë¡ì‹œ ì—­í• )
    else:
        return await handle_llm_proxy_request(req)


@router.post("/chat", response_model=ChatResponse, deprecated=True, summary="[Deprecated] Use /v1/chat/completions instead")
async def legacy_chat_endpoint(req: ChatRequest):
    """ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. /v1/chat/completionsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""
    state = {"messages": [msg.model_dump() for msg in req.messages]}
    result = await _agent.ainvoke(state)
    return ChatResponse(messages=result["messages"])


def create_chat_router(agent, agent_model_id: str):
    """
    ì±„íŒ… ë¼ìš°í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ì™€ ëª¨ë¸ IDë¥¼ ì£¼ì…ë°›ìŠµë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
        
    Returns:
        APIRouter: ì„¤ì •ëœ ì±„íŒ… ë¼ìš°í„°
    """
    set_agent_info(agent, agent_model_id)
    return router