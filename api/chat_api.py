"""
ì±„íŒ… ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

import time
import uuid
import logging
import time
import uuid
import logging
from fastapi import APIRouter, HTTPException, Depends, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session

from core.schemas import ChatRequest, ChatResponse, OpenAIChatRequest, AiderChatRequest
from core.llm_client import client, get_client_for_model, get_model_info
from core.database import get_db
from services.chat_service import get_chat_service
from utils.streaming_utils import agent_stream_generator, proxy_stream_generator
from tools.utils import find_last_user_message
import os # os ëª¨ë“ˆ ì„í¬íŠ¸
import requests # requests ëª¨ë“ˆ ì„í¬íŠ¸

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì •ë³´ ì €ì¥
_agent = None
_agent_model_id = None
_aider_agent = None
_aider_agent_model_id = None

router = APIRouter(
    tags=["ğŸ¤– AI Chat"],
    prefix="/v1",
    responses={
        200: {"description": "ì±„íŒ… ì‘ë‹µ ì„±ê³µ"},
        400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"description": "ì„œë²„ ì˜¤ë¥˜"}
    }
)


async def handle_agent_request(req: OpenAIChatRequest, agent, agent_model_id: str, 
                              request: Request, db: Session):
    """
    CoE ì—ì´ì „íŠ¸ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
        request: FastAPI Request ê°ì²´
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        
    Returns:
        ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ JSON ì‘ë‹µ
    """
    start_time = time.time()
    chat_service = get_chat_service(db)
    
    # ìš”ì²­ ë³¸ë¬¸ì—ì„œ session_id ì¶”ì¶œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    session_id = req.session_id
    user_agent = request.headers.get("User-Agent")
    ip_address = request.client.host if request.client else None
    
    # ì„¸ì…˜ ìƒì„± ë˜ëŠ” ì¡°íšŒ
    session = chat_service.get_or_create_session(
        session_id=session_id,
        user_agent=user_agent,
        ip_address=ip_address
    )
    current_session_id = session['session_id']
    
    # ë©”ì‹œì§€ ë‚´ìš©ì´ Noneì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
    sanitized_messages = []
    for msg in req.messages:
        msg_dump = msg.model_dump()
        if msg_dump.get("content") is None:
            msg_dump["content"] = ""
        sanitized_messages.append(msg_dump)

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    user_message = find_last_user_message(sanitized_messages)
    if user_message is not None:
        chat_service.save_chat_message(
            session_id=current_session_id,
            role="user",
            content=user_message,
            turn_number=session['conversation_turns'] + 1
        )
    
    # ì—ì´ì „íŠ¸ ìƒíƒœ ì¤€ë¹„
    state = {"messages": sanitized_messages}
    
    # ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰ ì •ë³´ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
    tool_context = {
        "session_id": current_session_id,
        "chat_service": chat_service,
        "turn_number": session['conversation_turns'] + 1
    }
    
    # ìƒíƒœì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    state["_tool_context"] = tool_context
    
    try:
        logger.info(f"Attempting to invoke agent with state: {state.keys()}")
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        result = await agent.ainvoke(state)
        logger.info(f"Agent invocation successful. Result keys: {result.keys()}")
        final_message = find_last_user_message(result["messages"], role="assistant")

        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time_ms = int((time.time() - start_time) * 1000)

        # final_messageê°€ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
        if final_message is None:
            final_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            logger.warning(f"ì—ì´ì „íŠ¸ê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. session_id={current_session_id}")

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ (ë„êµ¬ ì •ë³´ëŠ” tool_wrapperì—ì„œ ì²˜ë¦¬ë¨)
        chat_service.save_chat_message(
            session_id=current_session_id,
            role="assistant",
            content=final_message,
            turn_number=session['conversation_turns'] + 1
        )
        
        # ì„¸ì…˜ í„´ ìˆ˜ ì—…ë°ì´íŠ¸
        chat_service.update_session_turns(current_session_id)
        
        # API í˜¸ì¶œ ë¡œê·¸ ì €ì¥
        chat_service.log_api_call(
            session_id=current_session_id,
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=200,
            response_time_ms=response_time_ms
        )
        
        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            return StreamingResponse(
                agent_stream_generator(req.model, final_message, current_session_id),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": req.model,
                "choices": [{"index": 0, "message": {"role": "assistant", "content": final_message}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "session_id": current_session_id
            }
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹…
        response_time_ms = int((time.time() - start_time) * 1000)
        error_message = str(e)
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {error_message}", exc_info=True)
        
        chat_service.log_api_call(
            session_id=current_session_id,
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=500,
            response_time_ms=response_time_ms,
            error_message=error_message
        )
        
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {error_message}")
        raise HTTPException(status_code=500, detail=f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {error_message}")


async def handle_llm_proxy_request(req: OpenAIChatRequest):
    """
    ì¼ë°˜ LLM ëª¨ë¸ ìš”ì²­ì„ í”„ë¡ì‹œí•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        
    Returns:
        í”„ë¡ì‹œëœ LLM ì‘ë‹µ
    """
    try:
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = get_model_info(req.model)
        if not model_info:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {req.model}. ì§€ì› ëª¨ë¸ ëª©ë¡ì€ /v1/models ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
            )
        
        # í•´ë‹¹ ëª¨ë¸ì˜ í”„ë¡œë°”ì´ë”ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        model_client = get_client_for_model(req.model)
        
        # í”„ë¡œë°”ì´ë”ë³„ í´ë¼ì´ì–¸íŠ¸ë¡œ ìš”ì²­ì„ ì „ë‹¬
        response = model_client.chat.completions.create(
            model=req.model,
            messages=[msg.model_dump() for msg in req.messages],
            stream=req.stream,
            temperature=req.temperature,
            max_tokens=req.max_tokens
        )
        
        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í”„ë¡ì‹œ
            return StreamingResponse(
                proxy_stream_generator(response), 
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ í”„ë¡ì‹œ
            return response

    except ValueError as e:
        # ëª¨ë¸ ë˜ëŠ” í”„ë¡œë°”ì´ë” ê´€ë ¨ ì˜¤ë¥˜
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")


def set_agent_info(agent, agent_model_id: str):
    """
    ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ì „ì—­ ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
    """
    global _agent, _agent_model_id
    _agent = agent
    _agent_model_id = agent_model_id


def set_aider_agent_info(agent, agent_model_id: str):
    """
    Aider ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ì „ì—­ ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
    """
    global _aider_agent, _aider_agent_model_id
    _aider_agent = agent
    _aider_agent_model_id = agent_model_id


@router.post(
    "/chat/completions",
    summary="AI ì±„íŒ… API (OpenAI í˜¸í™˜)",
    description="""
    **OpenAIì˜ Chat Completions APIì™€ í˜¸í™˜ë˜ëŠ” AI ì—ì´ì „íŠ¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.**

    ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ìš”ì²­ëœ `model`ì— ë”°ë¼ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤:
    1.  **CoE ì—ì´ì „íŠ¸ ëª¨ë¸ (`coe-agent-v1`)**: LangGraphë¡œ êµ¬ì¶•ëœ ë‚´ë¶€ AI ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬, ë“±ë¡ëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    2.  **ì¼ë°˜ LLM ëª¨ë¸ (ì˜ˆ: `gpt-4o-mini`)**: ì™¸ë¶€ LLM API(OpenAI, Anthropic ë“±)ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” í”„ë¡ì‹œ ì—­í• ì„ í•©ë‹ˆë‹¤.

    ### ğŸ”„ ì„¸ì…˜ ê´€ë¦¬ (ëŒ€í™” ì—°ì†ì„±)
    - **ì²« ìš”ì²­**: `session_id` ì—†ì´ ìš”ì²­í•˜ë©´, ì„œë²„ê°€ ìƒˆë¡œìš´ `session_id`ë¥¼ ìƒì„±í•˜ì—¬ ì‘ë‹µ ë³¸ë¬¸ì— í¬í•¨í•´ ë°˜í™˜í•©ë‹ˆë‹¤.
    - **ëŒ€í™” ì´ì–´ê°€ê¸°**: ë‹¤ìŒ ìš”ì²­ë¶€í„°ëŠ” ì´ `session_id`ë¥¼ ìš”ì²­ ë³¸ë¬¸ì— í¬í•¨ì‹œì¼œ ë³´ë‚´ì•¼ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ### ğŸ¤– ì§€ì› ëª¨ë¸
    - `coe-agent-v1`: CoE LangGraph ì—ì´ì „íŠ¸ (ë‚´ë¶€ ë„êµ¬ ì‚¬ìš©)
    - `gpt-4o-mini`, `gpt-4o`: OpenAI ëª¨ë¸ í”„ë¡ì‹œ
    - `claude-3-sonnet-20240229`: Anthropic ëª¨ë¸ í”„ë¡ì‹œ
    - ì „ì²´ ëª©ë¡ì€ `/v1/models` APIë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ### ğŸ“ ì‚¬ìš© ì˜ˆì‹œ (cURL)
    ```bash
    # 1. ì²« ë²ˆì§¸ ìš”ì²­ (ì„¸ì…˜ ì‹œì‘)
    curl -X POST "http://localhost:8000/v1/chat/completions" \\
      -H "Content-Type: application/json" \\
      -d '{
        "model": "coe-agent-v1",
        "messages": [{"role": "user", "content": "LangGraphê°€ ë­ì•¼?"}]
      }' # ì‘ë‹µì—ì„œ session_id í™•ì¸

    # 2. ë‘ ë²ˆì§¸ ìš”ì²­ (ëŒ€í™” ì´ì–´ê°€ê¸°)
    curl -X POST "http://localhost:8000/v1/chat/completions" \\
      -H "Content-Type: application/json" \\
      -d '{
        "model": "coe-agent-v1",
        "session_id": "ì—¬ê¸°ì—_ë°›ì€_ì„¸ì…˜IDë¥¼_ì…ë ¥í•˜ì„¸ìš”",
        "messages": [{"role": "user", "content": "ê·¸ê²ƒì˜ ì¥ì ì€ ë­ì•¼?"}]
      }'
    ```
    """,
    response_description="ì±„íŒ… ì™„ì„± ì‘ë‹µ. ìŠ¤íŠ¸ë¦¬ë° ì‹œ `text/event-stream`, ì•„ë‹ ê²½ìš° `application/json`."
)
@router.post(
    "/chat/completions/aider",
    summary="AI ì±„íŒ… API (aider ì „ìš©, RAG ê·¸ë£¹ í•„í„°ë§)",
    description="""
    **aiderì™€ ê°™ì€ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ OpenAI Chat Completions API í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.**
    `group_name`ì„ ì‚¬ìš©í•˜ì—¬ RAG(Retrieval-Augmented Generation) ê²€ìƒ‰ì„ íŠ¹ì • ê·¸ë£¹ì˜ ë°ì´í„°ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.

    ### ğŸ”„ ì„¸ì…˜ ê´€ë¦¬ (ëŒ€í™” ì—°ì†ì„±)
    - **ì²« ìš”ì²­**: `session_id` ì—†ì´ ìš”ì²­í•˜ë©´, ì„œë²„ê°€ ìƒˆë¡œìš´ `session_id`ë¥¼ ìƒì„±í•˜ì—¬ ì‘ë‹µ ë³¸ë¬¸ì— í¬í•¨í•´ ë°˜í™˜í•©ë‹ˆë‹¤.
    - **ëŒ€í™” ì´ì–´ê°€ê¸°**: ë‹¤ìŒ ìš”ì²­ë¶€í„°ëŠ” ì´ `session_id`ë¥¼ ìš”ì²­ ë³¸ë¬¸ì— í¬í•¨ì‹œì¼œ ë³´ë‚´ì•¼ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ### ğŸ¤– ì§€ì› ëª¨ë¸
    - `ax4`: sk Adot 4 ëª¨ë¸ ì‚¬ìš©
    - `gpt-4o-mini`, `gpt-4o`: OpenAI ëª¨ë¸ í”„ë¡ì‹œ
    - ì „ì²´ ëª©ë¡ì€ `/v1/models` APIë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ### ğŸ“ ì‚¬ìš© ì˜ˆì‹œ (cURL)
    ```bash
    # 1. ì²« ë²ˆì§¸ ìš”ì²­ (ì„¸ì…˜ ì‹œì‘, group_name í¬í•¨)
    curl -X POST "http://localhost:8000/v1/chat/completions/aider" \
      -H "Content-Type: application/json" \
      -d '{ 
        "model": "ax4",
        "messages": [{"role": "user", "content": "payment ëª¨ë“ˆì˜ ì½”ë“œ êµ¬ì¡°ì— ëŒ€í•´ ì•Œë ¤ì¤˜"}],
        "group_name": "swing"
      }' # ì‘ë‹µì—ì„œ session_id í™•ì¸

    # 2. ë‘ ë²ˆì§¸ ìš”ì²­ (ëŒ€í™” ì´ì–´ê°€ê¸°, group_name í¬í•¨)
    curl -X POST "http://localhost:8000/v1/chat/completions/aider" \
      -H "Content-Type: application/json" \
      -d '{ 
        "model": "ax4",
        "session_id": "ì—¬ê¸°ì—_ë°›ì€_ì„¸ì…˜IDë¥¼_ì…ë ¥í•˜ì„¸ìš”",
        "messages": [{"role": "user", "content": "ê·¸ê²ƒì˜ ì¥ì ì€ ë­ì•¼?"}],
        "group_name": "swing"
      }'
    ```
    """,
    response_description="ì±„íŒ… ì™„ì„± ì‘ë‹µ. ìŠ¤íŠ¸ë¦¬ë° ì‹œ `text/event-stream`, ì•„ë‹ ê²½ìš° `application/json`."
)
async def chat_completions_aider(req: AiderChatRequest, request: Request, db: Session = Depends(get_db)):
    """OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ - CoE ì—ì´ì „íŠ¸ ë˜ëŠ” ì™¸ë¶€ LLM í˜¸ì¶œ (RAG ê·¸ë£¹ í•„í„°ë§)"""
    logger.info(f"ì±„íŒ… ìš”ì²­ ìˆ˜ì‹  (aider ì „ìš©): model={req.model}, messages={len(req.messages)}, session_id={req.session_id}, group_name={req.group_name})")

    # RAG ê²€ìƒ‰ ë¡œì§
    rag_context = ""
    user_message_content = find_last_user_message(req.messages)

    if req.group_name is None:
        req.group_name = "swing"

    if user_message_content and req.group_name:
        # RAG ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆì˜ì¸ì§€ íŒë‹¨ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        rag_keywords = ["ì½”ë“œ", "ë¶„ì„", "ì •ë³´", "êµ¬ì¡°", "ê¸°ëŠ¥", "ì–´ë–»ê²Œ", "ì„¤ëª…", "ì˜ˆì‹œ", "ëª¨ë“ˆ", "í´ë˜ìŠ¤", "í•¨ìˆ˜", "íŒŒì¼"]
        if any(keyword in user_message_content for keyword in rag_keywords):
            try:
                rag_pipeline_base_url = os.getenv("RAG_PIPELINE_BASE_URL", "http://localhost:8001")
                rag_search_url = f"{rag_pipeline_base_url}/api/v1/search"

                search_payload = {
                    "query": user_message_content,
                    "k": 3, # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰
                    "filter_metadata": {"group_name": req.group_name}
                }
                
                logger.info(f"RAG ê²€ìƒ‰ ìš”ì²­: {rag_search_url}, payload: {search_payload}")
                rag_response = requests.post(rag_search_url, json=search_payload)
                rag_response.raise_for_status()
                
                rag_results = rag_response.json()
                
                if rag_results:
                    rag_context = "\n\n--- ê´€ë ¨ ì½”ë“œ/ë¬¸ì„œ ---\n"
                    for doc in rag_results:
                        rag_context += f"íŒŒì¼: {doc['metadata'].get('file_path', 'N/A')}\n"
                        rag_context += f"ë‚´ìš©: {doc['content']}\n---\n"
                    logger.info(f"RAG ê²€ìƒ‰ ê²°ê³¼ {len(rag_results)}ê°œ ë°œê²¬.")
                else:
                    logger.info("RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")

            except Exception as e:
                logger.error(f"RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                rag_context = f"RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # ì›ë³¸ ë©”ì‹œì§€ì— RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    messages_to_process = req.messages
    if rag_context:
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        for i in range(len(messages_to_process) - 1, -1, -1):
            if messages_to_process[i].role == "user":
                messages_to_process[i].content += rag_context
                break
        logger.info("RAG ì»¨í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ì ë©”ì‹œì§€ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        # RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°, ì—ì´ì „íŠ¸ì—ê²Œ ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€
        logger.info("RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
        for i in range(len(messages_to_process) - 1, -1, -1):
            if messages_to_process[i].role == "user":
                messages_to_process[i].content += "\n\n[ì¤‘ìš”]: ì €ì¥ëœ ë¬¸ì„œì—ì„œ ìš”ì²­í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ëŠ” ë°”ê°€ ì—†ë‹¤ë©´ 'ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ê±°ë‚˜, ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì ˆëŒ€ë¡œ ì—†ëŠ” ì •ë³´ë¥¼ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”."
                break

    # req ê°ì²´ì˜ messagesë¥¼ ì—…ë°ì´íŠ¸ëœ ë©”ì‹œì§€ë¡œ êµì²´
    req.messages = messages_to_process

    # 1. CoE ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš°
    if req.model == _aider_agent_model_id:
        return await handle_agent_request(req, _aider_agent, _aider_agent_model_id, request, db)
    # 2. ì¼ë°˜ LLM ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš° (í”„ë¡ì‹œ ì—­í• )
    else:
        return await handle_llm_proxy_request(req)



@router.post("/chat", response_model=ChatResponse, deprecated=True, summary="[Deprecated] Use /v1/chat/completions instead")
async def legacy_chat_endpoint(req: ChatRequest):
    """ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. /v1/chat/completionsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""
    state = {"messages": [msg.model_dump() for msg in req.messages]}
    result = await _agent.ainvoke(state)
    return ChatResponse(messages=result["messages"])


def create_chat_router():
    """
    ì±„íŒ… ë¼ìš°í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Returns:
        APIRouter: ì„¤ì •ëœ ì±„íŒ… ë¼ìš°í„°
    """
    return router