"""
ì±„íŒ… ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

import time
import uuid
import logging
import time
import uuid
import logging
from fastapi import APIRouter, HTTPException, Depends, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session

from core.schemas import ChatRequest, ChatResponse, OpenAIChatRequest
from core.llm_client import client, get_client_for_model, get_model_info
from core.database import get_db
from services.chat_service import get_chat_service
from utils.streaming_utils import agent_stream_generator, proxy_stream_generator
from tools.utils import find_last_user_message

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì •ë³´ ì €ì¥
_agent = None
_agent_model_id = None

router = APIRouter(
    tags=["ğŸ¤– AI Chat"],
    prefix="/v1",
    responses={
        200: {"description": "ì±„íŒ… ì‘ë‹µ ì„±ê³µ"},
        400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"description": "ì„œë²„ ì˜¤ë¥˜"}
    }
)


async def handle_agent_request(req: OpenAIChatRequest, agent, agent_model_id: str, 
                              request: Request, db: Session):
    """
    CoE ì—ì´ì „íŠ¸ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
        request: FastAPI Request ê°ì²´
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        
    Returns:
        ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ JSON ì‘ë‹µ
    """
    start_time = time.time()
    chat_service = get_chat_service(db) # Pass the db session here
    
    # ì„¸ì…˜ ID ì¶”ì¶œ (í—¤ë”ì—ì„œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±)
    session_id = request.headers.get("X-Session-ID")
    user_agent = request.headers.get("User-Agent")
    ip_address = request.client.host if request.client else None
    
    # ì„¸ì…˜ ìƒì„± ë˜ëŠ” ì¡°íšŒ
    session = chat_service.get_or_create_session(
        session_id=session_id,
        user_agent=user_agent,
        ip_address=ip_address
    )
    
    # ë©”ì‹œì§€ ë‚´ìš©ì´ Noneì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
    sanitized_messages = []
    for msg in req.messages:
        msg_dump = msg.model_dump()
        if msg_dump.get("content") is None:
            msg_dump["content"] = ""
        sanitized_messages.append(msg_dump)

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    user_message = find_last_user_message(sanitized_messages)
    if user_message is not None:
        chat_service.save_chat_message(
            session_id=session['session_id'],
            role="user",
            content=user_message,
            turn_number=session['conversation_turns'] + 1
        )
    
    # ì—ì´ì „íŠ¸ ìƒíƒœ ì¤€ë¹„
    state = {"messages": sanitized_messages}
    
    # ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰ ì •ë³´ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
    tool_context = {
        "session_id": session['session_id'],
        "chat_service": chat_service,
        "turn_number": session['conversation_turns'] + 1
    }
    
    # ìƒíƒœì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    state["_tool_context"] = tool_context
    
    try:
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        print("DEBUG: Attempting to invoke agent.")
        result = await agent.ainvoke(state)
        final_message = find_last_user_message(result["messages"], role="assistant")

        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time_ms = int((time.time() - start_time) * 1000)

        # final_messageê°€ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
        if final_message is None:
            final_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            logger.warning(f"ì—ì´ì „íŠ¸ê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. session_id={session['session_id']}")

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ (ë„êµ¬ ì •ë³´ëŠ” tool_wrapperì—ì„œ ì²˜ë¦¬ë¨)
        chat_service.save_chat_message(
            session_id=session['session_id'],
            role="assistant",
            content=final_message,
            turn_number=session['conversation_turns'] + 1
        )
        
        # ì„¸ì…˜ í„´ ìˆ˜ ì—…ë°ì´íŠ¸
        chat_service.update_session_turns(session['session_id'])
        
        # API í˜¸ì¶œ ë¡œê·¸ ì €ì¥
        chat_service.log_api_call(
            session_id=session['session_id'],
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=200,
            response_time_ms=response_time_ms
        )
        
        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            return StreamingResponse(
                agent_stream_generator(req.model, final_message),
                media_type="text/event-stream",
                headers={"X-Session-ID": session['session_id']}
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": req.model,
                "choices": [{"index": 0, "message": {"role": "assistant", "content": final_message}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},  # ì‚¬ìš©ëŸ‰ì€ ì¶”ì í•˜ì§€ ì•ŠìŒ
                "x_session_id": session['session_id']  # ì„¸ì…˜ ID ë°˜í™˜
            }
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹…
        response_time_ms = int((time.time() - start_time) * 1000)
        error_message = str(e)
        
        chat_service.log_api_call(
            session_id=session['session_id'],
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=500,
            response_time_ms=response_time_ms,
            error_message=error_message
        )
        
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {error_message}")
        raise HTTPException(status_code=500, detail=f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {error_message}")


async def handle_llm_proxy_request(req: OpenAIChatRequest):
    """
    ì¼ë°˜ LLM ëª¨ë¸ ìš”ì²­ì„ í”„ë¡ì‹œí•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        
    Returns:
        í”„ë¡ì‹œëœ LLM ì‘ë‹µ
    """
    try:
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = get_model_info(req.model)
        if not model_info:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {req.model}. ì§€ì› ëª¨ë¸ ëª©ë¡ì€ /v1/models ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
            )
        
        # í•´ë‹¹ ëª¨ë¸ì˜ í”„ë¡œë°”ì´ë”ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        model_client = get_client_for_model(req.model)
        
        # í”„ë¡œë°”ì´ë”ë³„ í´ë¼ì´ì–¸íŠ¸ë¡œ ìš”ì²­ì„ ì „ë‹¬
        response = model_client.chat.completions.create(
            model=req.model,
            messages=[msg.model_dump() for msg in req.messages],
            stream=req.stream,
            temperature=req.temperature,
            max_tokens=req.max_tokens
        )
        
        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í”„ë¡ì‹œ
            return StreamingResponse(
                proxy_stream_generator(response), 
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ í”„ë¡ì‹œ
            return response.model_dump()

    except ValueError as e:
        # ëª¨ë¸ ë˜ëŠ” í”„ë¡œë°”ì´ë” ê´€ë ¨ ì˜¤ë¥˜
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")


def set_agent_info(agent, agent_model_id: str):
    """
    ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ì „ì—­ ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
    """
    global _agent, _agent_model_id
    _agent = agent
    _agent_model_id = agent_model_id


@router.post(
    "/chat/completions",
    summary="AI ì±„íŒ… ì™„ì„±",
    description="""
    **OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.**
    
    ìš”ì²­ëœ ëª¨ë¸ IDì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤:
    - **CoE ì—ì´ì „íŠ¸ ëª¨ë¸**: LangGraph ê¸°ë°˜ AI ì—ì´ì „íŠ¸ ì‹¤í–‰
    - **ì¼ë°˜ LLM ëª¨ë¸**: OpenAI/Anthropic ë“± ì™¸ë¶€ LLM API í”„ë¡ì‹œ
    
    ### ğŸ¤– ì§€ì› ëª¨ë¸
    - `coe-agent-v1`: CoE LangGraph ì—ì´ì „íŠ¸ (ì¶”ì²œ)
    - `gpt-4`, `gpt-3.5-turbo`: OpenAI ëª¨ë¸
    - `claude-3-sonnet`: Anthropic ëª¨ë¸
    
    ### ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    ```bash
    curl -X POST "http://localhost:8000/v1/chat/completions" \\
      -H "Content-Type: application/json" \\
      -H "X-Session-ID: your-session-id" \\
      -d '{
        "model": "coe-agent-v1",
        "messages": [
          {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! CoE ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤."}
        ],
        "stream": false
      }'
    ```
    
    ### ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    `"stream": true` ì„¤ì •ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### ğŸ“Š ì„¸ì…˜ ê´€ë¦¬
    `X-Session-ID` í—¤ë”ë¡œ ì„¸ì…˜ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—†ìœ¼ë©´ ìƒˆ ì„¸ì…˜ì´ ìƒì„±ë©ë‹ˆë‹¤.
    """,
    response_description="ì±„íŒ… ì™„ì„± ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” JSON)"
)
async def chat_completions(req: OpenAIChatRequest, request: Request, db: Session = Depends(get_db)):
    """OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ - CoE ì—ì´ì „íŠ¸ ë˜ëŠ” ì™¸ë¶€ LLM í˜¸ì¶œ"""
    logger.info(f"ì±„íŒ… ìš”ì²­ ìˆ˜ì‹ : model={req.model}, messages={len(req.messages)}")
    
    # 1. CoE ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš°
    if req.model == _agent_model_id:
        return await handle_agent_request(req, _agent, _agent_model_id, request, db)
    # 2. ì¼ë°˜ LLM ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš° (í”„ë¡ì‹œ ì—­í• )
    else:
        return await handle_llm_proxy_request(req)


@router.post("/chat", response_model=ChatResponse, deprecated=True, summary="[Deprecated] Use /v1/chat/completions instead")
async def legacy_chat_endpoint(req: ChatRequest):
    """ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. /v1/chat/completionsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""
    state = {"messages": [msg.model_dump() for msg in req.messages]}
    result = await _agent.ainvoke(state)
    return ChatResponse(messages=result["messages"])


def create_chat_router(agent, agent_model_id: str):
    """
    ì±„íŒ… ë¼ìš°í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ì™€ ëª¨ë¸ IDë¥¼ ì£¼ì…ë°›ìŠµë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
        
    Returns:
        APIRouter: ì„¤ì •ëœ ì±„íŒ… ë¼ìš°í„°
    """
    set_agent_info(agent, agent_model_id)
    return router