"""
ì±„íŒ… ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

import time
import uuid
import logging
import re # re ëª¨ë“ˆ ì¶”ê°€
import httpx # httpx ëª¨ë“ˆ ì¶”ê°€
from fastapi import APIRouter, HTTPException, Depends, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from typing import Optional # Optional íƒ€ì… ì¶”ê°€

from core.schemas import ChatRequest, ChatResponse, OpenAIChatRequest
from core.llm_client import client, get_client_for_model, get_model_info
from core.database import get_db
from services.chat_service import get_chat_service
from utils.streaming_utils import agent_stream_generator, proxy_stream_generator
from tools.utils import find_last_user_message

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì •ë³´ ì €ì¥
_agent = None
_agent_model_id = None

router = APIRouter(
    tags=["ğŸ¤– AI Chat"],
    prefix="/v1",
    responses={
        200: {"description": "ì±„íŒ… ì‘ë‹µ ì„±ê³µ"},
        400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"description": "ì„œë²„ ì˜¤ë¥˜"}
    }
)

def extract_git_url(text: str) -> Optional[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ Git URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ê°„ë‹¨í•œ Git URL íŒ¨í„´ ë§¤ì¹­ (ë” ì •êµí•œ íŒ¨í„´ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
    match = re.search(r'https://github\.com/[\w\-\.#/]+', text)
    if match:
        return match.group(0)
    return None

async def trigger_rag_analysis(git_url: str) -> Optional[str]:
    """CoE-RagPipelineì— Git ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤."""
    rag_pipeline_url = "http://127.0.0.1:8001" # CoE-RagPipeline URL
    analyze_url = f"{rag_pipeline_url}/api/v1/analyze"
    
    try:
        async with httpx.AsyncClient() as client:
            payload = {
                "repositories": [
                    {
                        "url": git_url,
                        "branch": "master"
                    }
                ],
                "include_ast": True,
                "include_tech_spec": True,
                "include_correlation": True
            }
            response = await client.post(analyze_url, json=payload, timeout=300) # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
            
            result = response.json()
            return result.get("analysis_id")
    except httpx.RequestError as e:
        logger.error(f"CoE-RagPipeline ë¶„ì„ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        logger.error(f"CoE-RagPipeline ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None

async def handle_agent_request(req: OpenAIChatRequest, agent, agent_model_id: str, 
                              request: Request, db: Session):
    """
    CoE ì—ì´ì „íŠ¸ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
        request: FastAPI Request ê°ì²´
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        
    Returns:
        ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ JSON ì‘ë‹µ
    """
    start_time = time.time()
    chat_service = get_chat_service(db) # Pass the db session here
    
    # ì„¸ì…˜ ID ì¶”ì¶œ (í—¤ë”ì—ì„œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±)
    session_id = request.headers.get("X-Session-ID")
    user_agent = request.headers.get("User-Agent")
    ip_address = request.client.host if request.client else None
    
    # ì„¸ì…˜ ìƒì„± ë˜ëŠ” ì¡°íšŒ
    session = chat_service.get_or_create_session(
        session_id=session_id,
        user_agent=user_agent,
        ip_address=ip_address
    )
    
    # ë©”ì‹œì§€ ë‚´ìš©ì´ Noneì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
    sanitized_messages = []
    for msg in req.messages:
        msg_dump = msg.model_dump()
        if msg_dump.get("content") is None:
            msg_dump["content"] = ""
        sanitized_messages.append(msg_dump)

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    user_message_content = find_last_user_message(sanitized_messages)
    if user_message_content is not None:
        chat_service.save_chat_message(
            session_id=session['session_id'],
            role="user",
            content=user_message_content,
            turn_number=session['conversation_turns'] + 1
        )
    
    # Git URL ê°ì§€ ë° RAG ë¶„ì„ íŠ¸ë¦¬ê±°
    git_url = extract_git_url(user_message_content)
    if git_url:
        logger.info(f"Git URL ê°ì§€ë¨: {git_url}. CoE-RagPipeline ë¶„ì„ ìš”ì²­.")
        analysis_id = await trigger_rag_analysis(git_url)
        
        if analysis_id:
            response_content = (
                f"Git ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤: {git_url}\n"
                f"ë¶„ì„ ID: `{analysis_id}`\n"
                f"ë¶„ì„ì´ ì™„ë£Œë˜ë©´ ì´ IDë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì´ë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                f"ì˜ˆì‹œ: `analysis_id {analysis_id} ë¡œ ê°œë°œ ê°€ì´ë“œë¥¼ ì¶”ì¶œí•´ì¤˜`"
            )
        else:
            response_content = (
                f"Git ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {git_url}\n"
                f"CoE-RagPipeline ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
        
        # ì‚¬ìš©ìì—ê²Œ ë¶„ì„ ì‹œì‘ ë©”ì‹œì§€ ì¦‰ì‹œ ë°˜í™˜
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": req.model,
            "choices": [{"index": 0, "message": {"role": "assistant", "content": response_content}, "finish_reason": "stop"}],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            "x_session_id": session['session_id']
        }

    # ì—ì´ì „íŠ¸ ìƒíƒœ ì¤€ë¹„
    state = {"messages": sanitized_messages}
    
    # ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰ ì •ë³´ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
    tool_context = {
        "session_id": session['session_id'],
        "chat_service": chat_service,
        "turn_number": session['conversation_turns'] + 1
    }
    
    # ìƒíƒœì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    state["_tool_context"] = tool_context
    
    try:
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        print("DEBUG: Attempting to invoke agent.")
        result = await agent.ainvoke(state)
        final_message = find_last_user_message(result["messages"], role="assistant")

        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time_ms = int((time.time() - start_time) * 1000)

        # final_messageê°€ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
        if final_message is None:
            final_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            logger.warning(f"ì—ì´ì „íŠ¸ê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. session_id={session['session_id']}")

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ (ë„êµ¬ ì •ë³´ëŠ” tool_wrapperì—ì„œ ì²˜ë¦¬ë¨)
        chat_service.save_chat_message(
            session_id=session['session_id'],
            role="assistant",
            content=final_message,
            turn_number=session['conversation_turns'] + 1
        )
        
        # ì„¸ì…˜ í„´ ìˆ˜ ì—…ë°ì´íŠ¸
        chat_service.update_session_turns(session['session_id'])
        
        # API í˜¸ì¶œ ë¡œê·¸ ì €ì¥
        chat_service.log_api_call(
            session_id=session['session_id'],
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=200,
            response_time_ms=response_time_ms
        )
        
        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            return StreamingResponse(
                agent_stream_generator(req.model, final_message),
                media_type="text/event-stream",
                headers={"X-Session-ID": session['session_id']}
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": req.model,
                "choices": [{"index": 0, "message": {"role": "assistant", "content": final_message}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},  # ì‚¬ìš©ëŸ‰ì€ ì¶”ì í•˜ì§€ ì•ŠìŒ
                "x_session_id": session['session_id']  # ì„¸ì…˜ ID ë°˜í™˜
            }
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹…
        response_time_ms = int((time.time() - start_time) * 1000)
        error_message = str(e)
        
        chat_service.log_api_call(
            session_id=session['session_id'],
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=500,
            response_time_ms=response_time_ms,
            error_message=error_message
        )
        
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {error_message}")
        raise HTTPException(status_code=500, detail=f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {error_message}")


async def handle_llm_proxy_request(req: OpenAIChatRequest):
    """
    ì¼ë°˜ LLM ëª¨ë¸ ìš”ì²­ì„ í”„ë¡ì‹œí•©ë‹ˆë‹¤.
    
    Args:
        req: OpenAI í˜¸í™˜ ì±„íŒ… ìš”ì²­
        
    Returns:
        í”„ë¡ì‹œëœ LLM ì‘ë‹µ
    """
    try:
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = get_model_info(req.model)
        if not model_info:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {req.model}. ì§€ì› ëª¨ë¸ ëª©ë¡ì€ /v1/models ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
            )
        
        # í•´ë‹¹ ëª¨ë¸ì˜ í”„ë¡œë°”ì´ë”ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        model_client = get_client_for_model(req.model)
        
        # í”„ë¡œë°”ì´ë”ë³„ í´ë¼ì´ì–¸íŠ¸ë¡œ ìš”ì²­ì„ ì „ë‹¬
        response = model_client.chat.completions.create(
            model=req.model,
            messages=[msg.model_dump() for msg in req.messages],
            stream=req.stream,
            temperature=req.temperature,
            max_tokens=req.max_tokens
        )
        
        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í”„ë¡ì‹œ
            return StreamingResponse(
                proxy_stream_generator(response), 
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ í”„ë¡ì‹œ
            return response.model_dump()

    except ValueError as e:
        # ëª¨ë¸ ë˜ëŠ” í”„ë¡œë°”ì´ë” ê´€ë ¨ ì˜¤ë¥˜
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")


def set_agent_info(agent, agent_model_id: str):
    """
    ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ì „ì—­ ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
    """
    global _agent, _agent_model_id
    _agent = agent
    _agent_model_id = agent_model_id


@router.post(
    "/chat/completions",
    summary="AI ì±„íŒ… ì™„ì„±",
    description="""
    **OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.**
    
    ìš”ì²­ëœ ëª¨ë¸ IDì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤:
    - **CoE ì—ì´ì „íŠ¸ ëª¨ë¸**: LangGraph ê¸°ë°˜ AI ì—ì´ì „íŠ¸ ì‹¤í–‰
    - **ì¼ë°˜ LLM ëª¨ë¸**: OpenAI/Anthropic ë“± ì™¸ë¶€ LLM API í”„ë¡ì‹œ
    
    ### ğŸ¤– ì§€ì› ëª¨ë¸
    - `coe-agent-v1`: CoE LangGraph ì—ì´ì „íŠ¸ (ì¶”ì²œ)
    - `gpt-4`, `gpt-3.5-turbo`: OpenAI ëª¨ë¸
    - `claude-3-sonnet`: Anthropic ëª¨ë¸
    
    ### ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    ```bash
    curl -X POST "http://localhost:8000/v1/chat/completions" \\
      -H "Content-Type: application/json" \\
      -H "X-Session-ID: your-session-id" \\
      -d '{
        "model": "coe-agent-v1",
        "messages": [
          {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! CoE ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤."}
        ],
        "stream": false
      }'
    ```
    
    ### ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    `"stream": true` ì„¤ì •ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### ğŸ“Š ì„¸ì…˜ ê´€ë¦¬
    `X-Session-ID` í—¤ë”ë¡œ ì„¸ì…˜ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—†ìœ¼ë©´ ìƒˆ ì„¸ì…˜ì´ ìƒì„±ë©ë‹ˆë‹¤.
    """,
    response_description="ì±„íŒ… ì™„ì„± ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” JSON)"
)
async def chat_completions(req: OpenAIChatRequest, request: Request, db: Session = Depends(get_db)):
    """OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ - CoE ì—ì´ì „íŠ¸ ë˜ëŠ” ì™¸ë¶€ LLM í˜¸ì¶œ"""
    logger.info(f"ì±„íŒ… ìš”ì²­ ìˆ˜ì‹ : model={req.model}, messages={len(req.messages)}")
    
    # 1. CoE ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš°
    if req.model == _agent_model_id:
        return await handle_agent_request(req, _agent, _agent_model_id, request, db)
    # 2. ì¼ë°˜ LLM ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš° (í”„ë¡ì‹œ ì—­í• )
    else:
        return await handle_llm_proxy_request(req)


@router.post("/chat", response_model=ChatResponse, deprecated=True, summary="[Deprecated] Use /v1/chat/completions instead")
async def legacy_chat_endpoint(req: ChatRequest):
    """ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. /v1/chat/completionsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""
    state = {"messages": [msg.model_dump() for msg in req.messages]}
    result = await _agent.ainvoke(state)
    return ChatResponse(messages=result["messages"])


def create_chat_router(agent, agent_model_id: str):
    """
    ì±„íŒ… ë¼ìš°í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ì™€ ëª¨ë¸ IDë¥¼ ì£¼ì…ë°›ìŠµë‹ˆë‹¤.
    
    Args:
        agent: ì»´íŒŒì¼ëœ LangGraph ì—ì´ì „íŠ¸
        agent_model_id: ì—ì´ì „íŠ¸ ëª¨ë¸ ID
        
    Returns:
        APIRouter: ì„¤ì •ëœ ì±„íŒ… ë¼ìš°í„°
    """
    set_agent_info(agent, agent_model_id)
    return router