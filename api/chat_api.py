"""
ì±„íŒ… ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

import time
import uuid
import logging
import json
from fastapi import APIRouter, HTTPException, Depends, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session

from core.schemas import (
    OpenAIChatRequest, AiderChatRequest, AgentState
)
from core.llm_client import get_client_for_model, get_model_info
from core.database import get_db
from services.chat_service import get_chat_service, ChatService
from utils.streaming_utils import agent_stream_generator, proxy_stream_generator

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì •ë³´ ì €ì¥
_agent = None
_agent_model_id = None
_aider_agent = None
_aider_agent_model_id = None

router = APIRouter(
    tags=["ğŸ¤– AI Chat"],
    prefix="/v1",
)

def set_agent_info(agent, agent_model_id: str):
    global _agent, _agent_model_id
    _agent = agent
    _agent_model_id = agent_model_id

def set_aider_agent_info(agent, agent_model_id: str):
    global _aider_agent, _aider_agent_model_id
    _aider_agent = agent
    _aider_agent_model_id = agent_model_id

async def handle_agent_request(req: OpenAIChatRequest, agent, agent_model_id: str, 
                              request: Request, db: Session):
    """
    ìƒˆë¡œìš´ Modal Context Protocol ì—ì´ì „íŠ¸ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    chat_service = get_chat_service(db)

    # 1. ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
    session = chat_service.get_or_create_session(
        session_id=req.session_id,
        user_agent=request.headers.get("User-Agent"),
        ip_address=request.client.host if request.client else None
    )
    current_session_id = session['session_id']

    # 2. ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜¤ê³  í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    raw_history = chat_service.get_chat_history(current_session_id, limit=20)
    history_dicts = []
    if not raw_history and not req.messages:
         # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì—¬ê¸°ì— ì¶”ê°€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        history_dicts.append({"role": "system", "content": "You are a helpful assistant."})

    for msg in raw_history:
        history_dicts.append({"role": msg.role, "content": msg.content})
    
    current_user_content = ""
    for msg in req.messages:
        message_dump = msg.model_dump(exclude_none=True)
        history_dicts.append(message_dump)
        if msg.role == 'user':
            current_user_content = msg.content

    # 3. ìƒˆë¡œìš´ AgentStateë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    # TODO: 'mode'ë¥¼ ì‚¬ìš©ì ìš”ì²­ì´ë‚˜ ì„¸ì…˜ ìƒíƒœì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    agent_state = AgentState(
        history=history_dicts,
        input=current_user_content,
        mode="coding", # í˜„ì¬ëŠ” 'coding' ëª¨ë“œë¡œ ê³ ì •
        scratchpad={},
        session_id=current_session_id,
    )

    try:
        # 4. ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        logger.info(f"Invoking new agent with state: mode='{agent_state['mode']}', history_len={len(agent_state['history'])}")
        result_state = await agent.ainvoke(agent_state)
        logger.info("New agent invocation successful.")

        # 5. ê²°ê³¼ì—ì„œ ìµœì¢… ì‘ë‹µì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        final_message_dict = result_state["history"][-1]
        final_message_content = final_message_dict.get("content", "")

        # 6. DBì— ë©”ì‹œì§€ ì €ì¥ ë° ë¡œê¹…
        chat_service.save_chat_message(
            session_id=current_session_id,
            role="user",
            content=current_user_content,
            turn_number=session['conversation_turns'] + 1
        )
        chat_service.save_chat_message(
            session_id=current_session_id,
            role="assistant",
            content=final_message_content,
            turn_number=session['conversation_turns'] + 1
        )
        chat_service.update_session_turns(current_session_id)
        
        response_time_ms = int((time.time() - start_time) * 1000)
        chat_service.log_api_call(
            session_id=current_session_id,
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=200,
            response_time_ms=response_time_ms
        )

        # 7. ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ ì‘ë‹µ ë°˜í™˜
        if req.stream:
            return StreamingResponse(
                agent_stream_generator(req.model, final_message_content, current_session_id),
                media_type="text/event-stream"
            )
        else:
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": req.model,
                "choices": [{"index": 0, "message": final_message_dict, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "session_id": current_session_id
            }

    except Exception as e:
        response_time_ms = int((time.time() - start_time) * 1000)
        error_message = str(e)
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {error_message}", exc_info=True)
        
        chat_service.log_api_call(
            session_id=current_session_id,
            endpoint="/v1/chat/completions",
            method="POST",
            request_data={"model": req.model, "message_count": len(req.messages)},
            response_status=500,
            response_time_ms=response_time_ms,
            error_message=error_message
        )
        
        raise HTTPException(status_code=500, detail=f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {error_message}")

async def handle_llm_proxy_request(req: OpenAIChatRequest):
    """
    ì¼ë°˜ LLM ëª¨ë¸ ìš”ì²­ì„ í”„ë¡ì‹œí•©ë‹ˆë‹¤.
    """
    try:
        model_info = get_model_info(req.model)
        if not model_info:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {req.model}. ì§€ì› ëª¨ë¸ ëª©ë¡ì€ /v1/models ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
            )
        
        model_client = get_client_for_model(req.model)
        
        params = {
            "model": req.model,
            "messages": [msg.model_dump(exclude_none=True) for msg in req.messages],
            "stream": req.stream,
            "temperature": req.temperature,
            "max_tokens": req.max_tokens,
        }
        
        if req.tools:
            params["tools"] = [t.model_dump(exclude_none=True) for t in req.tools]
        if req.tool_choice:
            params["tool_choice"] = req.tool_choice

        response = model_client.chat.completions.create(**params)
        
        if req.stream:
            return StreamingResponse(
                proxy_stream_generator(response), 
                media_type="text/event-stream"
            )
        else:
            return response.model_dump(exclude_none=True)

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"LLM API í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"LLM API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")

@router.post("/chat/completions")
async def chat_completions(req: OpenAIChatRequest, request: Request, db: Session = Depends(get_db)):
    """AI ì—ì´ì „íŠ¸ ë˜ëŠ” ì¼ë°˜ LLM í”„ë¡ì‹œë¥¼ í†µí•´ ì±„íŒ… ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    
    agent_model_ids = [_agent_model_id, _aider_agent_model_id]

    if req.model in agent_model_ids:
        # ìš”ì²­ëœ ëª¨ë¸ì´ ë“±ë¡ëœ ì—ì´ì „íŠ¸ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ ê²½ìš°
        agent_to_use = _agent if req.model == _agent_model_id else _aider_agent
        return await handle_agent_request(req, agent_to_use, req.model, request, db)
    else:
        # ë“±ë¡ëœ ì—ì´ì „íŠ¸ ëª¨ë¸ì´ ì•„ë‹ˆë©´ LLM í”„ë¡ì‹œë¡œ ì²˜ë¦¬
        return await handle_llm_proxy_request(req)
