"""
Ï±ÑÌåÖ Í¥ÄÎ†® API ÏóîÎìúÌè¨Ïù∏Ìä∏Îì§ÏùÑ Îã¥ÎãπÌïòÎäî Î™®ÎìàÏûÖÎãàÎã§.

Î¶¨Ìå©ÌÜ†ÎßÅ ÌïµÏã¨
- context(ÌîÑÎ°†Ìä∏ UI Íµ¨Î∂ÑÏûê)Ïóê ÎßûÏ∂∞ ÏÑúÎ≤Ñ ÎèÑÍµ¨ ÏÑ∏Ìä∏Î•º Î°úÎìú(get_available_tools_for_context)
- ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï†úÍ≥µ toolsÏôÄ Î≥ëÌï© ÌõÑ LangGraph AgentÏóê Ï†ÑÎã¨
- LLM ÌîÑÎ°ùÏãú Ìò∏Ï∂ú Ïãú tools/tool_choiceÏùò None Ï†úÍ±∞ Î∞è ÍµêÏ∞® Í≤ÄÏ¶ù (ÌÉÄÏÇ¨ OpenAI Ìò∏Ìôò Í≤åÏù¥Ìä∏Ïõ®Ïù¥ 400 Î∞©ÏßÄ)
- BadRequestÎäî Í∞ÄÎä•ÌïòÎ©¥ 400ÏúºÎ°ú Ìå®Ïä§Ïä§Î£®, Í∏∞ÌÉÄ ÏòàÏô∏Îäî 500
"""

import time
import uuid
import logging
import httpx
import re
from datetime import datetime
import json
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, HTTPException, Depends, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session

from core.schemas import OpenAIChatRequest, AgentState
from core.llm_client import get_client_for_model, get_model_info
from core.database import get_db
from services.chat_service import get_chat_service, ChatService
from services.pii_service import scrub_text
from utils.streaming_utils import agent_stream_generator, proxy_stream_generator

# (ÏÑ†ÌÉù) OpenAI SDK BadRequest Ìå®Ïä§Ïä§Î£®Ïö©
try:
    import openai  # type: ignore
    OPENAI_IMPORTED = True
except Exception:
    OPENAI_IMPORTED = False

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

router = APIRouter(tags=["ü§ñ AI Chat"], prefix="/v1")

_agent_instance = None


# -----------------------------
# Agent Ï†ïÎ≥¥ Îì±Î°ù/Ï°∞Ìöå
# -----------------------------
def set_agent_info(agent, agent_model_id: str):
    global _agent_instance
    _agent_instance = {"agent": agent, "model_id": agent_model_id}


async def get_agent_info():
    if _agent_instance is None:
        raise HTTPException(status_code=500, detail="Agent not initialized")
    return _agent_instance


# -----------------------------
# Ïú†Ìã∏
# -----------------------------
def _drop_none_fields(d: Dict[str, Any]) -> Dict[str, Any]:
    """dictÏóêÏÑú None Í∞í ÌÇ§ Ï†úÍ±∞"""
    return {k: v for k, v in d.items() if v is not None}


def _tool_key_for_merge(t: Any) -> str:
    """
    ÎèÑÍµ¨ Ï§ëÎ≥µ Ï†úÍ±∞ ÌÇ§ ÏÉùÏÑ±:
    - pydantic Í∞ùÏ≤¥: name ÏÜçÏÑ±
    - OpenAI tools(dict): function.name
    - dictÏóê name: Í∑∏ Í∞í
    - Í∑∏ Ïô∏: id(t)
    """
    try:
        if hasattr(t, "name"):
            return getattr(t, "name")
        if isinstance(t, dict):
            if "function" in t and isinstance(t["function"], dict):
                return t["function"].get("name") or str(id(t))
            if "name" in t:
                return t.get("name") or str(id(t))
    except Exception:
        pass
    return str(id(t))


def _shorten_for_log(text: Optional[str], limit: int = 400) -> str:
    """Collapse whitespace and cap length for log safety."""
    if not text:
        return ""
    compact = re.sub(r"\s+", " ", text).strip()
    if len(compact) <= limit:
        return compact
    overflow = len(compact) - limit
    return f"{compact[:limit]}‚Ä¶(+{overflow} chars)"


def _merge_tool_schemas(server_schemas: List[Any], client_schemas: Optional[List[Any]]) -> List[Any]:
    """
    ÏÑúÎ≤ÑÏóêÏÑú Î°úÎìúÌïú ÎèÑÍµ¨ Ïä§ÌÇ§Îßà + ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÎèÑÍµ¨ Ïä§ÌÇ§Îßà Î≥ëÌï©(Ï§ëÎ≥µ Ï†úÍ±∞)
    """
    client_schemas = client_schemas or []
    merged: Dict[str, Any] = {}
    for t in server_schemas + client_schemas:
        merged[_tool_key_for_merge(t)] = t
    return list(merged.values())

async def _get_or_create_session_and_history(
    req: OpenAIChatRequest, chat_service: ChatService, request: Request
):

    session = chat_service.get_or_create_session(
        session_id=req.session_id,
        user_agent=request.headers.get("User-Agent"),
        ip_address=request.client.host if request.client else None,
    )
    current_session_id = session["session_id"]

    raw_history = chat_service.get_chat_history(current_session_id, limit=20)
    history_dicts: List[Dict[str, Any]] = []
    if not req.messages:
        history_dicts.append({"role": "system", "content": "You are a helpful assistant."})

    current_user_content = ""
    for msg in raw_history:
        history_dicts.append({"role": msg.role, "content": msg.content})

    for msg in req.messages:
        message_dump = msg.model_dump(exclude_none=True)
        history_dicts.append(message_dump)
        if msg.role == "user":
            current_user_content = msg.content

    return session, current_session_id, history_dicts, current_user_content


async def _log_and_save_messages(
    chat_service: ChatService,
    current_session_id: str,
    current_user_content: str,
    final_message_content: str,
    session: dict,
    start_time: float,
    req: OpenAIChatRequest,
    response_status: int,
    error_message: str = None,
):
    turn = session["conversation_turns"] + 1
    chat_service.save_chat_message(
        session_id=current_session_id, role="user", content=current_user_content, turn_number=turn
    )
    chat_service.save_chat_message(
        session_id=current_session_id, role="assistant", content=final_message_content, turn_number=turn
    )
    chat_service.update_session_turns(current_session_id)

    response_time_ms = int((time.time() - start_time) * 1000)
    chat_service.log_api_call(
        session_id=current_session_id,
        endpoint="/v1/chat/completions",
        method="POST",
        request_data={
            "model": req.model,
            "message_count": len(req.messages),
            "context": req.context,
        },
        response_status=response_status,
        response_time_ms=response_time_ms,
        error_message=error_message,
    )


# -----------------------------
# LangGraph Agent ÏöîÏ≤≠ Ï≤òÎ¶¨
# -----------------------------
async def handle_agent_request(
    req: OpenAIChatRequest,
    agent,
    agent_model_id: str,
    request: Request,
    db: Session,
):
    logger.debug("Entering handle_agent_request function.")
    """
    LangGraph ÏóêÏù¥Ï†ÑÌä∏ ÏöîÏ≤≠ Ï≤òÎ¶¨
    - context(UI Íµ¨Î∂ÑÏûê)Ïóê ÎßûÏ∂∞ ÏÑúÎ≤Ñ ÎèÑÍµ¨ ÏÑ∏Ìä∏ Î°úÎìú(get_available_tools_for_context)
    - ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï†úÍ≥µ toolsÏôÄ Î≥ëÌï©ÌïòÏó¨ AgentÏóê Ï†ÑÎã¨
    """
    start_time = time.time()
    chat_service = get_chat_service(db)

    session, current_session_id, history_dicts, current_user_content = await _get_or_create_session_and_history(
        req, chat_service, request
    )

    masked_user_content, pii_hits = scrub_text(current_user_content)
    if pii_hits:
        logger.info(
            "[PII] session=%s masked_types=%s",
            current_session_id,
            ",".join(sorted({hit["type"] for hit in pii_hits})),
        )

    # 1) context Í∏∞Î∞ò ÏÑúÎ≤Ñ ÎèÑÍµ¨ Ïä§ÌÇ§Îßà Î°úÎìú
    server_schemas: List[Any] = []
    try:
        from services import tool_dispatcher  # ÌîÑÎ°úÏ†ùÌä∏Ïùò dispatcher

        if hasattr(tool_dispatcher, "get_available_tools_for_context"):
            # (schemas, functions) ÌäúÌîåÏùÑ Î∞òÌôòÌïòÎØÄÎ°ú schemasÎßå ÏÇ¨Ïö©
            server_schemas, _functions = tool_dispatcher.get_available_tools_for_context(
                req.context or "",
                req.group_name,
            )
            logger.info(
                f"[TOOLS] context='{req.context}' ‚Üí server_schemas={len(server_schemas)}"
            )
        else:
            logger.info("tool_dispatcher.get_available_tools_for_context Í∞Ä ÏóÜÏñ¥ ÏÑúÎ≤Ñ ÎèÑÍµ¨ Î°úÎî©ÏùÑ ÏÉùÎûµÌï©ÎãàÎã§.")
    except Exception as e:
        logger.warning(f"ÏÑúÎ≤Ñ ÎèÑÍµ¨ Î°úÎî© Ïã§Ìå®(context='{req.context}'): {e}")

    # 2) ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÎèÑÍµ¨ÏôÄ Î≥ëÌï©
    resolved_tools = _merge_tool_schemas(server_schemas, req.tools)

    agent_state = AgentState(
        history=history_dicts,
        input=current_user_content,
        mode="coding",  # ÌòÑÏû¨Îäî Í≥†Ï†ï
        scratchpad={},
        session_id=current_session_id,
        model_id=req.model,
        group_name=req.group_name,
        tool_input=req.tool_input,
        context=req.context, # UI Ïª®ÌÖçÏä§Ìä∏: ÏóêÏù¥Ï†ÑÌä∏ ÎÇ¥Î∂Ä Î∂ÑÍ∏∞/Î°úÍπÖÏóê ÌôúÏö©
        tools=resolved_tools,
        requested_tool_choice=req.tool_choice,
    )

    logger.info(
        "[CHAT][REQUEST] session=%s context=%s group=%s model=%s user=%s",
        current_session_id,
        req.context or "",
        req.group_name or "",
        req.model,
        _shorten_for_log(masked_user_content),
    )

    try:
        # --- Fallback to agent flow (tool dispatcher handles auto-routing) ---
        logger.info(
            "Invoking agent | model='%s' context='%s' history=%d tools=%d",
            agent_state["model_id"],
            agent_state.get("context"),
            len(agent_state["history"]),
            len(agent_state.get("tools") or []),
        )
        result_state = await agent.ainvoke(agent_state)
        logger.info("Agent invocation successful.")

        final_message_dict = result_state["history"][-1]
        final_message_content = final_message_dict.get("content") or ""

        await _log_and_save_messages(
            chat_service,
            current_session_id,
            masked_user_content,
            final_message_content,
            session,
            start_time,
            req,
            200,
        )

        if req.stream:
            return StreamingResponse(
                agent_stream_generator(req.model, final_message_dict, current_session_id),
                media_type="text/event-stream",
            )
        else:
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": req.model,
                "choices": [{"index": 0, "message": final_message_dict, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "session_id": current_session_id,
            }

    except Exception as e:
        error_message = str(e)
        logger.error("ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ Ï§ë ÏòàÏô∏ Î∞úÏÉù: %s: %s", type(e).__name__, error_message, exc_info=True)

        await _log_and_save_messages(
            chat_service, current_session_id, masked_user_content, "", session, start_time, req, 500, error_message
        )

        raise HTTPException(status_code=500, detail=f"ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ Ïò§Î•ò: {error_message}")


# -----------------------------
# ÏùºÎ∞ò LLM ÌîÑÎ°ùÏãú
# -----------------------------
async def handle_llm_proxy_request(req: OpenAIChatRequest):
    """
    ÏùºÎ∞ò LLM Î™®Îç∏ ÏöîÏ≤≠ ÌîÑÎ°ùÏãú
    - tools/tool_choice None Ï†úÍ±∞
    - tool_choiceÎßå ÏûàÍ≥† tools ÏóÜÎäî Í≤ΩÏö∞ Ï†úÍ±∞(ÎòêÎäî Î¨¥Ïãú)
    - ÏùºÎ∂Ä OpenAI Ìò∏Ìôò Í≤åÏù¥Ìä∏Ïõ®Ïù¥Ïùò ÏóÑÍ≤© Í≤ÄÏ¶ùÏúºÎ°ú Ïù∏Ìïú 400 Î∞©ÏßÄ
    """
    try:
        model_info = get_model_info(req.model)
        if not model_info:
            raise HTTPException(
                status_code=400,
                detail=f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î™®Îç∏ÏûÖÎãàÎã§: {req.model}. ÏßÄÏõê Î™®Îç∏ Î™©Î°ùÏùÄ /v1/models ÏóîÎìúÌè¨Ïù∏Ìä∏ÏóêÏÑú ÌôïÏù∏ÌïòÏÑ∏Ïöî.",
            )

        model_client = get_client_for_model(req.model)

        last_user_message = ""
        for msg in reversed(req.messages or []):
            if msg.role == "user":
                last_user_message = msg.content
                break
        logger.info(
            "[CHAT][LLM REQUEST] model=%s user=%s",
            req.model,
            _shorten_for_log(last_user_message),
        )

        params: Dict[str, Any] = {
            "model": req.model,
            "messages": [msg.model_dump(exclude_none=True) for msg in req.messages],
            "stream": req.stream,
            "temperature": req.temperature,
            "max_tokens": req.max_tokens,
        }

        # tools / tool_choice ÏÑ§Ï†ï (None Í∏àÏßÄ)
        if req.tools:
            params["tools"] = [t.model_dump(exclude_none=True) for t in req.tools]

        if req.tool_choice is not None:
            if "tools" in params:
                params["tool_choice"] = req.tool_choice
            else:
                logger.warning("tool_choice provided without tools ‚Üí drop tool_choice")

        # ÏµúÏ¢Ö None Ï†úÍ±∞
        params = _drop_none_fields(params)

        logger.debug(f"[LLM Proxy] effective params keys: {list(params.keys())}")

        response = model_client.chat.completions.create(**params)

        if req.stream:
            return StreamingResponse(
                proxy_stream_generator(response),
                media_type="text/event-stream",
            )
        else:
            return response.model_dump(exclude_none=True)

    except HTTPException:
        raise
    except Exception as e:
        # OpenAI SDK BadRequestÎ•º 400ÏúºÎ°ú Ìå®Ïä§Ïä§Î£® (Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
        if OPENAI_IMPORTED and isinstance(e, getattr(openai, "BadRequestError", Exception)):
            detail = str(e)
            try:
                detail = getattr(e, "response", None) or getattr(e, "body", None) or detail
            except Exception:
                pass
            raise HTTPException(status_code=400, detail=f"Upstream 400: {detail}")

        logger.error("LLM API Ìò∏Ï∂ú Ïò§Î•ò: %s", e, exc_info=True)
        raise HTTPException(status_code=500, detail=f"LLM API Ìò∏Ï∂ú Ïò§Î•ò: {str(e)}")


# -----------------------------
# (ÏòµÏÖò) RAG Ïó∞Í≥Ñ
# -----------------------------
import os as _os
_RAG_BASE = _os.getenv("RAG_PIPELINE_URL", "http://ragpipeline:8001")
RAG_SERVICE_URL = f"{_RAG_BASE}/api/v1/search"
RAG_SCORE_THRESHOLD = 0.7


async def handle_rag_request(req: OpenAIChatRequest, request: Request, db: Session, agent_info: dict):
    """
    group_nameÏù¥ ÏûàÎäî Í≤ΩÏö∞ RAG ÏöîÏ≤≠ Ï≤òÎ¶¨
    """
    start_time = time.time()
    chat_service = get_chat_service(db)

    session, current_session_id, history_dicts, current_user_content = await _get_or_create_session_and_history(
        req, chat_service, request
    )

    masked_user_content, rag_pii_hits = scrub_text(current_user_content)
    if rag_pii_hits:
        logger.info(
            "[PII] session=%s masked_types=%s",
            current_session_id,
            ",".join(sorted({hit["type"] for hit in rag_pii_hits})),
        )

    logger.info(
        "[CHAT][RAG REQUEST] session=%s context=%s group=%s model=%s user=%s",
        current_session_id,
        req.context or "",
        req.group_name or "",
        req.model,
        _shorten_for_log(masked_user_content),
    )

    user_query = current_user_content
    retrieved_documents: List[Dict[str, Any]] = []

    try:
        async with httpx.AsyncClient() as client:
            rag_request_payload = {"query": user_query, "k": 10, "group_name": req.group_name}
            logger.info(f"Calling RAG service at {RAG_SERVICE_URL} with payload: {rag_request_payload}")
            response = await client.post(RAG_SERVICE_URL, json=rag_request_payload, timeout=300.0)
            response.raise_for_status()
            retrieved_documents = response.json()
            logger.info(f"Received {len(retrieved_documents)} documents from RAG service.")
    except httpx.RequestError as e:
        error_message = f"RAG ÏÑúÎπÑÏä§ Ìò∏Ï∂ú Ïã§Ìå®: {e}"
        logger.error(error_message, exc_info=True)
        await _log_and_save_messages(
            chat_service, current_session_id, masked_user_content, "", session, start_time, req, 500, error_message
        )
        raise HTTPException(status_code=500, detail=error_message)
    except httpx.HTTPStatusError as e:
        error_message = f"RAG ÏÑúÎπÑÏä§ ÏùëÎãµ Ïò§Î•ò: {e.response.status_code} - {e.response.text}"
        logger.error(error_message, exc_info=True)
        await _log_and_save_messages(
            chat_service, current_session_id, masked_user_content, "", session, start_time, req, 500, error_message
        )
        raise HTTPException(status_code=500, detail=error_message)
    except Exception as e:
        error_message = f"RAG Ï≤òÎ¶¨ Ï§ë Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò Î∞úÏÉù: {e}"
        logger.error(error_message, exc_info=True)
        await _log_and_save_messages(
            chat_service, current_session_id, masked_user_content, "", session, start_time, req, 500, error_message
        )
        raise HTTPException(status_code=500, detail=error_message)

    satisfactory_documents = [doc for doc in retrieved_documents if doc.get("rerank_score", 0) >= RAG_SCORE_THRESHOLD]

    if not satisfactory_documents:
        logger.warning(
            f"No satisfactory documents retrieved from RAG for query: {user_query}. Falling back to agent/LLM proxy logic."
        )
        agent = agent_info["agent"]
        if req.model == agent_info["model_id"]:
            return await handle_agent_request(req, agent, req.model, request, db)
        else:
            return await handle_llm_proxy_request(req)

    context = "\n\n".join([doc["content"] for doc in satisfactory_documents])

    messages_for_llm = [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant. Use the following context to answer the user's question. "
                "If the context does not provide enough information, state that you cannot answer based on the provided context.\n\n"
                f"Context:\n{context}"
            ),
        },
        {"role": "user", "content": user_query},
    ]

    try:
        model_client = get_client_for_model(req.model)
        llm_params: Dict[str, Any] = {
            "model": req.model,
            "messages": messages_for_llm,
            "stream": req.stream,
            "temperature": req.temperature,
            "max_tokens": req.max_tokens,
        }
        llm_params = _drop_none_fields(llm_params)

        llm_response = model_client.chat.completions.create(**llm_params)

        if req.stream:
            return StreamingResponse(proxy_stream_generator(llm_response), media_type="text/event-stream")
        else:
            final_message_dict = llm_response.model_dump(exclude_none=True)["choices"][0]["message"]
            final_message_content = final_message_dict.get("content", "")

            await _log_and_save_messages(
                chat_service,
                current_session_id,
                masked_user_content,
                final_message_content,
                session,
                start_time,
                req,
                200,
            )

            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": req.model,
                "choices": [{"index": 0, "message": final_message_dict, "finish_reason": "stop"}],
                "usage": llm_response.usage.model_dump()
                if getattr(llm_response, "usage", None)
                else {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "session_id": current_session_id,
            }

    except Exception as e:
        error_message = str(e)
        logger.error("RAG LLM Ìò∏Ï∂ú Ïò§Î•ò: %s: %s", type(e).__name__, error_message, exc_info=True)

        await _log_and_save_messages(
            chat_service, current_session_id, masked_user_content, "", session, start_time, req, 500, error_message
        )

        raise HTTPException(status_code=500, detail=f"RAG LLM Ìò∏Ï∂ú Ïò§Î•ò: {error_message}")


# -----------------------------
# ÏóîÎìúÌè¨Ïù∏Ìä∏
# -----------------------------
@router.post("/chat/completions")
@router.post("/completions")
async def chat_completions(
    req: OpenAIChatRequest,
    request: Request,
    db: Session = Depends(get_db),
    agent_info: dict = Depends(get_agent_info),
):
    """
    Í∏∞Î≥∏Ï†ÅÏúºÎ°ú LangGraph AgentÎ°ú Ï≤òÎ¶¨
    (ÏõêÌïòÎ©¥ Ï°∞Í±¥Î∂ÄÎ°ú RAG/LLM ÌîÑÎ°ùÏãú Î∂ÑÍ∏∞ Ï∂îÍ∞Ä Í∞ÄÎä•)
    """
    agent = agent_info["agent"]
    return await handle_agent_request(req, agent, req.model, request, db)
