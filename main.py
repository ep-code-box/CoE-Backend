import json
from typing import Dict
from fastapi import FastAPI
from fastapi.responses import JSONResponse
from langgraph.graph import StateGraph, START, END

# ë¶„ë¦¬ëœ ëª¨ë“ˆì—ì„œ ìŠ¤í‚¤ë§ˆì™€ ë„êµ¬ ë…¸ë“œ ê°€ì ¸ì˜¤ê¸°
from schemas import ChatState, ChatRequest, ChatResponse, Message
from llm_client import client, MODEL_NAME # LLM í´ë¼ì´ì–¸íŠ¸ì™€ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°

# ëª¨ë“  ë„êµ¬ ë…¸ë“œ ê°€ì ¸ì˜¤ê¸°
from tools.api_tool import api_call_node
from tools.class_tool import class_call_node, class_analysis_node
from tools.human_tool import human_approval_node
from tools.subgraph_tool import sub_graph_node
from tools.langchain_tool import langchain_chain_node
from tools.basic_tools import tool1_node, tool2_node
from tools.utils import find_last_user_message

# 3) ë¼ìš°í„° ë…¸ë“œ: LLMì— ë¶„ê¸° ìš”ì²­ (ëª¨ë“  ë„êµ¬ë¥¼ í¬í•¨í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ í™•ì¥)
def router_node(state: ChatState) -> dict:
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ original_inputì— ì €ì¥
    last_user_message = find_last_user_message(state["messages"]) # utilsì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜ ì‚¬ìš©

    prompt_messages = state["messages"] + [
        {"role":"system","content":
         """ì‚¬ìš©ìì˜ ìš”ì²­ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ë‹¤ìŒ ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.
         - 'tool1': í…ìŠ¤íŠ¸ë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
         - 'tool2': í…ìŠ¤íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
         - 'api_call': ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ì ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ì˜ˆ: "1ë²ˆ ì‚¬ìš©ì ì •ë³´ ì•Œë ¤ì¤˜")
         - 'class_call': í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ì˜ˆ: "ì´ ë¬¸ì¥ ë¶„ì„í•´ì¤˜")
         - 'sub_graph': ì¸ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ì˜ˆ: "ì•ˆë…•")
         - 'human_approval': ì‚¬ëŒì˜ ìŠ¹ì¸ì´ í•„ìš”í•œ ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤. (ì˜ˆ: "ì¤‘ìš”í•œ ì‘ì—… ìŠ¹ì¸í•´ì¤˜")
         - 'langchain_chain': í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤. (ì˜ˆ: "ì´ ê¸´ ê¸€ì„ ìš”ì•½í•´ì¤˜")
         - 'combined_tool': APIë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ í´ë˜ìŠ¤ë¡œ ë¶„ì„í•˜ëŠ” ì¡°í•© ì‘ì—…ì…ë‹ˆë‹¤. (ì˜ˆ: "1ë²ˆ ì‚¬ìš©ì ë°ì´í„° ë¶„ì„í•´ì¤˜")
         - 'end': ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ëë‚´ê³  ì‹¶ì–´í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

         ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤: {"next_tool": "ì„ íƒí•œ ë„êµ¬"}"""}
    ]
    try:
        resp = client.chat.completions.create(
            model=MODEL_NAME, # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì€ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
            messages=prompt_messages,
            response_format={"type": "json_object"} # JSON ëª¨ë“œ í™œì„±í™”
        )
        # OpenAI ê°ì²´ë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ íƒ€ì… ì¼ê´€ì„± ìœ ì§€
        response_message = resp.choices[0].message.model_dump()
        
        try:
            # LLM ì‘ë‹µ(JSON) íŒŒì‹±
            choice_json = json.loads(response_message["content"])
            choice = choice_json.get("next_tool")
            print(f"ğŸ¤–[Router]: LLMì´ ì„ íƒí•œ ë„êµ¬: {choice}")
            if choice not in ["tool1", "tool2", "api_call", "class_call", "sub_graph", "human_approval", "langchain_chain", "combined_tool", "end"]:
                raise ValueError(f"LLMì´ ìœ íš¨í•˜ì§€ ì•Šì€ ë„êµ¬({choice})ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            # ë‹¤ìŒ ë…¸ë“œë¥¼ ìƒíƒœì— ì €ì¥í•˜ê³ , LLMì˜ ì‘ë‹µë„ ê¸°ë¡ì— ì¶”ê°€
            return {"messages": [response_message], "next_node": choice, "original_input": last_user_message}
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  ê·¸ë˜í”„ ì¢…ë£Œ
            error_msg = f"ë¼ìš°í„°ê°€ LLM ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
            print(f"ğŸ¤–[Router]: Error - {error_msg}")
            return {"messages": [response_message, {"role": "system", "content": error_msg}], "next_node": "error", "original_input": last_user_message}

    except Exception as e:
        # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  ê·¸ë˜í”„ ì¢…ë£Œ
        error_msg = f"ë¼ìš°í„° API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
        print(f"ğŸ¤–[Router]: Error - {error_msg}")
        return {"messages": [{"role": "system", "content": error_msg}], "next_node": "error"}

# 5) ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼ (ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€)
graph = StateGraph(ChatState)

# ë¼ìš°í„°ì™€ ëª¨ë“  ë„êµ¬ ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€
graph.add_node("router", router_node)
graph.add_node("tool1", tool1_node)
graph.add_node("tool2", tool2_node)
graph.add_node("api_call", api_call_node)
graph.add_node("class_call", class_call_node)
graph.add_node("sub_graph", sub_graph_node)
graph.add_node("human_approval", human_approval_node)
graph.add_node("langchain_chain", langchain_chain_node)
graph.add_node("class_analysis", class_analysis_node)

graph.set_entry_point("router")

# 'router' ë…¸ë“œì˜ ê²°ê³¼('next_node' ìƒíƒœ)ì— ë”°ë¼ ë¶„ê¸°
graph.add_conditional_edges(
    "router",
    lambda state: state["next_node"],
    {
        "tool1": "tool1",
        "tool2": "tool2",
        "api_call": "api_call",
        "class_call": "class_call",
        "sub_graph": "sub_graph",
        "human_approval": "human_approval",
        "langchain_chain": "langchain_chain",
        "combined_tool": "api_call", # ì¡°í•© í˜¸ì¶œì˜ ì‹œì‘ì 
        "end": END,
        "error": END, # 'error'ì¼ ê²½ìš° ê·¸ë˜í”„ ì¢…ë£Œ
    }
)

# ì¡°í•© í˜¸ì¶œì„ ìœ„í•œ ì—£ì§€ ì—°ê²° (API í˜¸ì¶œ í›„ í´ë˜ìŠ¤ ë¶„ì„)
graph.add_edge("api_call", "class_analysis")
graph.add_edge("class_analysis", END)

# ë‹¨ì¼ ì‘ì—… ë…¸ë“œë“¤ì€ ëª¨ë‘ ì¢…ë£Œ(END)ë¡œ ì—°ê²°
graph.add_edge("tool1", END)
graph.add_edge("tool2", END)
graph.add_edge("class_call", END)
graph.add_edge("sub_graph", END)
graph.add_edge("human_approval", END) # Human-in-the-loopì€ ì—¬ê¸°ì„œ ì¤‘ë‹¨ë¨
graph.add_edge("langchain_chain", END)

agent = graph.compile(interrupt_after=["human_approval"])

# 7) FastAPI ì•± ìƒì„± ë° ì—”ë“œí¬ì¸íŠ¸
app = FastAPI()

@app.get("/v1/models")
async def list_models():
    """
    LangChainì˜ ChatOpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ëª¨ë¸ ëª©ë¡ì„ ì¡°íšŒí•  ë•Œ ì‚¬ìš©í•˜ëŠ”
    OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    return JSONResponse(content={
        "object": "list",
        "data": [{
            "id": MODEL_NAME,
            "object": "model",
            "created": 1686935002,
            "owned_by": "user"
        }]
    })

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    # Pydantic ëª¨ë¸ì„ ë‚´ë¶€ ìƒíƒœ(dict)ë¡œ ë³€í™˜
    state = {
        "messages": [msg.model_dump() for msg in req.messages],
    }
    # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
    result = await agent.ainvoke(state)
    return ChatResponse(messages=result["messages"])

if __name__ == "__main__":
    import uvicorn
    # app: FastAPI ì¸ìŠ¤í„´ìŠ¤
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == "__main__":
    import uvicorn
    # app: FastAPI ì¸ìŠ¤í„´ìŠ¤
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)