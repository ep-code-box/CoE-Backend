import json
import os
import uuid
from typing import Dict, AsyncGenerator, Any
import time
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from langgraph.graph import StateGraph, START, END
from dotenv import load_dotenv

# ë¶„ë¦¬ëœ ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
from schemas import (
    ChatState, ChatRequest, ChatResponse,
    OpenAIChatRequest,  # OpenAI í˜¸í™˜ ìš”ì²­ ìŠ¤í‚¤ë§ˆ ì¶”ê°€
    LangFlowJSON, SaveFlowRequest, FlowListResponse, ExecuteFlowRequest
)
from llm_client import client, default_model # LLM í´ë¼ì´ì–¸íŠ¸ì™€ ê¸°ë³¸ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
from tools.utils import find_last_user_message
from tools.registry import load_all_tools
from models import model_registry # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°€ì ¸ì˜¤ê¸°

# 1) ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ í†µí•´ ëª¨ë“  ë…¸ë“œ, ì„¤ëª…, ì—£ì§€ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ
all_nodes, all_tool_descriptions, all_special_edges = load_all_tools()

# 'end' ë„êµ¬ì— ëŒ€í•œ ì„¤ëª… ì¶”ê°€
all_tool_descriptions.append({
    "name": "end",
    "description": "ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ëë‚´ê³  ì‹¶ì–´í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
})

# 2) ë¼ìš°í„°ê°€ ì‚¬ìš©í•  ìœ íš¨í•œ ë„êµ¬ ì´ë¦„ ëª©ë¡ ë° OpenWebUI ì—°ë™ì„ ìœ„í•œ ì—ì´ì „íŠ¸ ëª¨ë¸ ì •ì˜
VALID_TOOL_NAMES = [tool['name'] for tool in all_tool_descriptions]
AGENT_MODEL_ID = "coe-agent-v1" # OpenWebUIì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ ID

# ì—ì´ì „íŠ¸ë¥¼ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë™ì ìœ¼ë¡œ ë“±ë¡
model_registry.register_model(
    model_id=AGENT_MODEL_ID,
    name="CoE Agent v1", # OpenWebUIì— í‘œì‹œë  ì´ë¦„
    provider="CoE",
    description="CoE LangGraph Agent for development guide extraction"
)

# 3) ë¼ìš°í„° ë…¸ë“œ: ë™ì ìœ¼ë¡œ ìƒì„±ëœ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
def router_node(state: ChatState) -> dict:
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ original_inputì— ì €ì¥
    last_user_message = find_last_user_message(state["messages"]) # utilsì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜ ì‚¬ìš©
    
    # ë™ì ìœ¼ë¡œ ë„êµ¬ ì„¤ëª… ëª©ë¡ ìƒì„±
    tool_descriptions_string = "\n".join(
        [f"- '{tool['name']}': {tool['description']}" for tool in all_tool_descriptions]
    )
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = f"""ì‚¬ìš©ìì˜ ìš”ì²­ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ë‹¤ìŒ ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.
{tool_descriptions_string}

íŠ¹ë³„ ê·œì¹™:
- ë§Œì•½ ë°”ë¡œ ì´ì „ì˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìŠ¹ì¸ì„ ìš”ì²­í•˜ëŠ” ë‚´ìš©ì´ê³  ì‚¬ìš©ìê°€ 'approve' ë˜ëŠ” 'reject'ì™€ ìœ ì‚¬í•œ ì‘ë‹µì„ í–ˆë‹¤ë©´, ë°˜ë“œì‹œ 'process_approval' ë„êµ¬ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤: {{"next_tool": "ì„ íƒí•œ ë„êµ¬"}}"""

    prompt_messages = state["messages"] + [
        {"role": "system", "content": system_prompt}
    ]
    try:
        resp = client.chat.completions.create(
            model=default_model.model_id, # ê¸°ë³¸ ëª¨ë¸ ID ì‚¬ìš©
            messages=prompt_messages,
            response_format={"type": "json_object"} # JSON ëª¨ë“œ í™œì„±í™”
        )
        # OpenAI ê°ì²´ë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ íƒ€ì… ì¼ê´€ì„± ìœ ì§€
        response_message = resp.choices[0].message.model_dump()

        try:
            # LLM ì‘ë‹µ(JSON) íŒŒì‹±
            choice_json = json.loads(response_message["content"])
            choice = choice_json.get("next_tool")
            print(f"ğŸ¤–[Router]: LLMì´ ì„ íƒí•œ ë„êµ¬: {choice}")
            if choice not in VALID_TOOL_NAMES: # ìœ íš¨í•œ ë„êµ¬ ì´ë¦„ ëª©ë¡ìœ¼ë¡œ ê²€ì‚¬
                raise ValueError(f"LLMì´ ìœ íš¨í•˜ì§€ ì•Šì€ ë„êµ¬({choice})ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            # ë‹¤ìŒ ë…¸ë“œë¥¼ ìƒíƒœì— ì €ì¥í•˜ê³ , LLMì˜ ì‘ë‹µë„ ê¸°ë¡ì— ì¶”ê°€
            return {"messages": [response_message], "next_node": choice, "original_input": last_user_message}
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  ê·¸ë˜í”„ ì¢…ë£Œ
            error_msg = f"ë¼ìš°í„°ê°€ LLM ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
            print(f"ğŸ¤–[Router]: Error - {error_msg}")
            return {"messages": [response_message, {"role": "system", "content": error_msg}], "next_node": "error"}

    except Exception as e:
        # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  ê·¸ë˜í”„ ì¢…ë£Œ
        error_msg = f"ë¼ìš°í„° API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
        print(f"ğŸ¤–[Router]: Error - {error_msg}")
        return {"messages": [{"role": "system", "content": error_msg}], "next_node": "error"}

# 5) ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼ (ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€)
graph = StateGraph(ChatState)

# ë¼ìš°í„° ë…¸ë“œ ì¶”ê°€
graph.add_node("router", router_node)

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ë¡œë“œí•œ ëª¨ë“  ë„êµ¬ ë…¸ë“œë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€
for name, node_func in all_nodes.items():
    graph.add_node(name, node_func)

# ê·¸ë˜í”„ì˜ ì‹œì‘ì ì„ ë¼ìš°í„°ë¡œ ì„¤ì •
graph.set_entry_point("router")

# ë¼ìš°í„°ì˜ ê²°ì •ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¡œ ë¶„ê¸°í•˜ë„ë¡ ë™ì ìœ¼ë¡œ ì—£ì§€ ë§¤í•‘ ìƒì„±
routable_tool_names = [tool['name'] for tool in all_tool_descriptions if tool['name'] != 'end']
edge_mapping = {name: name for name in routable_tool_names}
edge_mapping["combined_tool"] = "api_call"  # 'combined_tool'ì€ 'api_call'ë¡œ ì‹œì‘í•˜ëŠ” íŠ¹ë³„ ì¼€ì´ìŠ¤
edge_mapping["end"] = END
edge_mapping["error"] = END

graph.add_conditional_edges(
    "router",
    lambda state: state["next_node"],
    edge_mapping
)

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ë¡œë“œí•œ íŠ¹ë³„í•œ ì—£ì§€ë“¤ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€
special_edge_sources = set()
for edge_config in all_special_edges:
    source_node = edge_config["source"]
    special_edge_sources.add(source_node)
    if edge_config["type"] == "conditional":
        graph.add_conditional_edges(
            source_node,
            edge_config["condition"],
            edge_config["path_map"]
        )
    elif edge_config["type"] == "standard":
        graph.add_edge(source_node, edge_config["target"])

# íŠ¹ë³„í•œ ì—£ì§€ê°€ ì •ì˜ëœ ë…¸ë“œì™€ ë¼ìš°í„°ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ëª¨ë“  ë…¸ë“œëŠ” ì‘ì—… ì™„ë£Œ í›„ ì¢…ë£Œ(END)ë¡œ ì—°ê²°
nodes_with_special_outgoing_edges = special_edge_sources.union({"router"})
for node_name in all_nodes:
    if node_name not in nodes_with_special_outgoing_edges:
        graph.add_edge(node_name, END)

agent = graph.compile(interrupt_after=["human_approval"])

# 7) FastAPI ì•± ìƒì„± ë° ì—”ë“œí¬ì¸íŠ¸
app = FastAPI()

@app.get("/v1/models")
async def list_models():
    """
    models.jsonì— ì •ì˜ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    OpenAIì˜ /v1/models ì—”ë“œí¬ì¸íŠ¸ì™€ í˜¸í™˜ë˜ëŠ” í˜•ì‹ì…ë‹ˆë‹¤.
    """
    all_models = model_registry.get_models()
    model_data = [
        {"id": model.model_id, "object": "model", "created": 1686935002, "owned_by": model.provider}
        for model in all_models
    ]
    return JSONResponse(content={
        "object": "list",
        "data": model_data
    })


# --- OpenAI í˜¸í™˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ ---

def _create_openai_chunk(model_id: str, content: str, finish_reason: str = None) -> str:
    """OpenAI ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ì— ë§ëŠ” ì²­í¬(chunk)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    chunk_id = f"chatcmpl-{uuid.uuid4()}"
    response = {
        "id": chunk_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model_id,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason,
            }
        ],
    }
    return f"data: {json.dumps(response)}\n\n"

async def _agent_stream_generator(model_id: str, final_message: str) -> AsyncGenerator[str, None]:
    """LangGraph ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë¹„ë™ê¸° ìƒì„±ê¸°ì…ë‹ˆë‹¤."""
    # ë©”ì‹œì§€ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.
    words = final_message.split(' ')
    for word in words:
        yield _create_openai_chunk(model_id, f"{word} ")
        await asyncio.sleep(0.05) # ì¸ìœ„ì ì¸ ë”œë ˆì´ë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ ê·¹ëŒ€í™”
    
    # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¥¼ ì•Œë¦¬ëŠ” ë§ˆì§€ë§‰ ì²­í¬
    yield _create_openai_chunk(model_id, "", "stop")
    yield "data: [DONE]\n\n"


# --- ë©”ì¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (OpenAI í˜¸í™˜) ---

@app.post("/v1/chat/completions")
async def chat_completions(req: OpenAIChatRequest):
    """
    OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.
    ìš”ì²­ëœ ëª¨ë¸ IDì— ë”°ë¼ CoE ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê±°ë‚˜, ì¼ë°˜ LLM í˜¸ì¶œì„ í”„ë¡ì‹œí•©ë‹ˆë‹¤.
    """
    # 1. CoE ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš°
    if req.model == AGENT_MODEL_ID:
        state = {"messages": [msg.model_dump() for msg in req.messages]}
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        result = await agent.ainvoke(state)
        final_message = find_last_user_message(result["messages"], role="assistant")

        if req.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            return StreamingResponse(
                _agent_stream_generator(req.model, final_message),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ JSON ì‘ë‹µ
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": req.model,
                "choices": [{"index": 0, "message": {"role": "assistant", "content": final_message}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0} # ì‚¬ìš©ëŸ‰ì€ ì¶”ì í•˜ì§€ ì•ŠìŒ
            }

    # 2. ì¼ë°˜ LLM ëª¨ë¸ì„ ìš”ì²­í•œ ê²½ìš° (í”„ë¡ì‹œ ì—­í• )
    else:
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ ìš”ì²­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
            response = client.chat.completions.create(
                model=req.model,
                messages=[msg.model_dump() for msg in req.messages],
                stream=req.stream,
                temperature=req.temperature,
                max_tokens=req.max_tokens
            )
            
            if req.stream:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í”„ë¡ì‹œ
                async def stream_proxy():
                    for chunk in response:
                        yield f"data: {chunk.model_dump_json()}\n\n"
                return StreamingResponse(stream_proxy(), media_type="text/event-stream")
            else:
                # ì¼ë°˜ JSON ì‘ë‹µ í”„ë¡ì‹œ
                return response.model_dump()

        except Exception as e:
            raise HTTPException(status_code=500, detail=f"LLM API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")

# ê¸°ì¡´ /chat ì—”ë“œí¬ì¸íŠ¸ëŠ” ë‚´ë¶€ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìœ ì§€í•˜ê±°ë‚˜ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” OpenWebUIì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ /v1/chat/completionsë¥¼ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
@app.post("/chat", response_model=ChatResponse, deprecated=True, summary="[Deprecated] Use /v1/chat/completions instead")
async def legacy_chat_endpoint(req: ChatRequest):
    """ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. /v1/chat/completionsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""
    state = {"messages": [msg.model_dump() for msg in req.messages]}
    result = await agent.ainvoke(state)
    return ChatResponse(messages=result["messages"])

# LangFlow ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
@app.post("/flows/save")
async def save_flow(req: SaveFlowRequest):
    """LangFlow JSONì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # flows ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        flows_dir = "flows"
        os.makedirs(flows_dir, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        safe_name = "".join(c for c in req.name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        filename = f"{safe_name}.json"
        filepath = os.path.join(flows_dir, filename)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        flow_data = req.flow_data.model_dump()
        flow_data["saved_name"] = req.name
        if req.description:
            flow_data["description"] = req.description
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(flow_data, f, indent=2, ensure_ascii=False)
        
        return {"message": f"Flow '{req.name}' saved successfully", "filename": filename}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save flow: {str(e)}")

@app.get("/flows/list", response_model=FlowListResponse)
async def list_flows():
    """ì €ì¥ëœ LangFlow ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        flows_dir = "flows"
        flows = []
        
        if os.path.exists(flows_dir):
            for filename in os.listdir(flows_dir):
                if filename.endswith('.json'):
                    filepath = os.path.join(flows_dir, filename)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            flow_data = json.load(f)
                        
                        flows.append({
                            "name": flow_data.get("saved_name", filename[:-5]),
                            "id": flow_data.get("id", filename[:-5]),
                            "description": flow_data.get("description", ""),
                            "filename": filename
                        })
                    except Exception as e:
                        print(f"Error reading flow file {filename}: {e}")
                        continue
        
        return FlowListResponse(flows=flows)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list flows: {str(e)}")

@app.get("/flows/{flow_name}")
async def get_flow(flow_name: str):
    """íŠ¹ì • LangFlow JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        flows_dir = "flows"
        
        # íŒŒì¼ëª…ìœ¼ë¡œ ì§ì ‘ ì°¾ê¸°
        if flow_name.endswith('.json'):
            filename = flow_name
        else:
            filename = f"{flow_name}.json"
        
        filepath = os.path.join(flows_dir, filename)
        
        if not os.path.exists(filepath):
            # ì €ì¥ëœ ì´ë¦„ìœ¼ë¡œ ì°¾ê¸°
            for file in os.listdir(flows_dir):
                if file.endswith('.json'):
                    file_path = os.path.join(flows_dir, file)
                    with open(file_path, 'r', encoding='utf-8') as f:
                        flow_data = json.load(f)
                    if flow_data.get("saved_name") == flow_name:
                        filepath = file_path
                        break
            else:
                raise HTTPException(status_code=404, detail=f"Flow '{flow_name}' not found")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            flow_data = json.load(f)
        
        return flow_data
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get flow: {str(e)}")

@app.delete("/flows/{flow_name}")
async def delete_flow(flow_name: str):
    """ì €ì¥ëœ LangFlowë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        flows_dir = "flows"
        
        # íŒŒì¼ëª…ìœ¼ë¡œ ì§ì ‘ ì°¾ê¸°
        if flow_name.endswith('.json'):
            filename = flow_name
        else:
            filename = f"{flow_name}.json"
        
        filepath = os.path.join(flows_dir, filename)
        
        if not os.path.exists(filepath):
            # ì €ì¥ëœ ì´ë¦„ìœ¼ë¡œ ì°¾ê¸°
            for file in os.listdir(flows_dir):
                if file.endswith('.json'):
                    file_path = os.path.join(flows_dir, file)
                    with open(file_path, 'r', encoding='utf-8') as f:
                        flow_data = json.load(f)
                    if flow_data.get("saved_name") == flow_name:
                        filepath = file_path
                        break
            else:
                raise HTTPException(status_code=404, detail=f"Flow '{flow_name}' not found")
        
        os.remove(filepath)
        return {"message": f"Flow '{flow_name}' deleted successfully"}
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete flow: {str(e)}")

if __name__ == "__main__":
    import uvicorn

    # .env íŒŒì¼ ë¡œë“œ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ í•„ìš”)
    load_dotenv()

    # í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ ê°œë°œ ëª¨ë“œì™€ í”„ë¡œë•ì…˜ ëª¨ë“œë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
    # APP_ENVê°€ 'development'ì¼ ë•Œë§Œ hot-reloadingì„ í™œì„±í™”í•©ë‹ˆë‹¤.
    is_development = os.getenv("APP_ENV") == "development"

    print(f"ğŸš€ Starting server in {'development (hot-reload enabled)' if is_development else 'production'} mode.")

    uvicorn.run(
        "main:app",
        host="0.0.0.0", port=8000, reload=is_development
    )
