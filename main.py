import json
import os
from typing import Dict
from fastapi import FastAPI
from fastapi.responses import JSONResponse
from langgraph.graph import StateGraph, START, END
from dotenv import load_dotenv

# ë¶„ë¦¬ëœ ëª¨ë“ˆì—ì„œ ìŠ¤í‚¤ë§ˆì™€ ë„êµ¬ ë…¸ë“œ ê°€ì ¸ì˜¤ê¸°
from schemas import ChatState, ChatRequest, ChatResponse, Message
from llm_client import client, default_model # LLM í´ë¼ì´ì–¸íŠ¸ì™€ ê¸°ë³¸ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
from tools.utils import find_last_user_message
from tools.registry import load_all_tools
from models import model_registry # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°€ì ¸ì˜¤ê¸°

# 1) ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ í†µí•´ ëª¨ë“  ë…¸ë“œ, ì„¤ëª…, ì—£ì§€ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ
all_nodes, all_tool_descriptions, all_special_edges = load_all_tools()

# 'end' ë„êµ¬ì— ëŒ€í•œ ì„¤ëª… ì¶”ê°€
all_tool_descriptions.append({
    "name": "end",
    "description": "ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ëë‚´ê³  ì‹¶ì–´í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
})

# 2) ë¼ìš°í„°ê°€ ì‚¬ìš©í•  ìœ íš¨í•œ ë„êµ¬ ì´ë¦„ ëª©ë¡ ìƒì„±
VALID_TOOL_NAMES = [tool['name'] for tool in all_tool_descriptions]

# 3) ë¼ìš°í„° ë…¸ë“œ: ë™ì ìœ¼ë¡œ ìƒì„±ëœ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
def router_node(state: ChatState) -> dict:
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ original_inputì— ì €ì¥
    last_user_message = find_last_user_message(state["messages"]) # utilsì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜ ì‚¬ìš©
    
    # ë™ì ìœ¼ë¡œ ë„êµ¬ ì„¤ëª… ëª©ë¡ ìƒì„±
    tool_descriptions_string = "\n".join(
        [f"- '{tool['name']}': {tool['description']}" for tool in all_tool_descriptions]
    )
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = f"""ì‚¬ìš©ìì˜ ìš”ì²­ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ë‹¤ìŒ ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.
{tool_descriptions_string}

íŠ¹ë³„ ê·œì¹™:
- ë§Œì•½ ë°”ë¡œ ì´ì „ì˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìŠ¹ì¸ì„ ìš”ì²­í•˜ëŠ” ë‚´ìš©ì´ê³  ì‚¬ìš©ìê°€ 'approve' ë˜ëŠ” 'reject'ì™€ ìœ ì‚¬í•œ ì‘ë‹µì„ í–ˆë‹¤ë©´, ë°˜ë“œì‹œ 'process_approval' ë„êµ¬ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤: {{"next_tool": "ì„ íƒí•œ ë„êµ¬"}}"""

    prompt_messages = state["messages"] + [
        {"role": "system", "content": system_prompt}
    ]
    try:
        resp = client.chat.completions.create(
            model=default_model.model_id, # ê¸°ë³¸ ëª¨ë¸ ID ì‚¬ìš©
            messages=prompt_messages,
            response_format={"type": "json_object"} # JSON ëª¨ë“œ í™œì„±í™”
        )
        # OpenAI ê°ì²´ë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ íƒ€ì… ì¼ê´€ì„± ìœ ì§€
        response_message = resp.choices[0].message.model_dump()

        try:
            # LLM ì‘ë‹µ(JSON) íŒŒì‹±
            choice_json = json.loads(response_message["content"])
            choice = choice_json.get("next_tool")
            print(f"ğŸ¤–[Router]: LLMì´ ì„ íƒí•œ ë„êµ¬: {choice}")
            if choice not in VALID_TOOL_NAMES: # ìœ íš¨í•œ ë„êµ¬ ì´ë¦„ ëª©ë¡ìœ¼ë¡œ ê²€ì‚¬
                raise ValueError(f"LLMì´ ìœ íš¨í•˜ì§€ ì•Šì€ ë„êµ¬({choice})ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            # ë‹¤ìŒ ë…¸ë“œë¥¼ ìƒíƒœì— ì €ì¥í•˜ê³ , LLMì˜ ì‘ë‹µë„ ê¸°ë¡ì— ì¶”ê°€
            return {"messages": [response_message], "next_node": choice, "original_input": last_user_message}
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  ê·¸ë˜í”„ ì¢…ë£Œ
            error_msg = f"ë¼ìš°í„°ê°€ LLM ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
            print(f"ğŸ¤–[Router]: Error - {error_msg}")
            return {"messages": [response_message, {"role": "system", "content": error_msg}], "next_node": "error"}

    except Exception as e:
        # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê¸°ë¡í•˜ê³  ê·¸ë˜í”„ ì¢…ë£Œ
        error_msg = f"ë¼ìš°í„° API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
        print(f"ğŸ¤–[Router]: Error - {error_msg}")
        return {"messages": [{"role": "system", "content": error_msg}], "next_node": "error"}

# 5) ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼ (ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€)
graph = StateGraph(ChatState)

# ë¼ìš°í„° ë…¸ë“œ ì¶”ê°€
graph.add_node("router", router_node)

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ë¡œë“œí•œ ëª¨ë“  ë„êµ¬ ë…¸ë“œë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€
for name, node_func in all_nodes.items():
    graph.add_node(name, node_func)

# ê·¸ë˜í”„ì˜ ì‹œì‘ì ì„ ë¼ìš°í„°ë¡œ ì„¤ì •
graph.set_entry_point("router")

# ë¼ìš°í„°ì˜ ê²°ì •ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¡œ ë¶„ê¸°í•˜ë„ë¡ ë™ì ìœ¼ë¡œ ì—£ì§€ ë§¤í•‘ ìƒì„±
routable_tool_names = [tool['name'] for tool in all_tool_descriptions if tool['name'] != 'end']
edge_mapping = {name: name for name in routable_tool_names}
edge_mapping["combined_tool"] = "api_call"  # 'combined_tool'ì€ 'api_call'ë¡œ ì‹œì‘í•˜ëŠ” íŠ¹ë³„ ì¼€ì´ìŠ¤
edge_mapping["end"] = END
edge_mapping["error"] = END

graph.add_conditional_edges(
    "router",
    lambda state: state["next_node"],
    edge_mapping
)

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ë¡œë“œí•œ íŠ¹ë³„í•œ ì—£ì§€ë“¤ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€
special_edge_sources = set()
for edge_config in all_special_edges:
    source_node = edge_config["source"]
    special_edge_sources.add(source_node)
    if edge_config["type"] == "conditional":
        graph.add_conditional_edges(
            source_node,
            edge_config["condition"],
            edge_config["path_map"]
        )
    elif edge_config["type"] == "standard":
        graph.add_edge(source_node, edge_config["target"])

# íŠ¹ë³„í•œ ì—£ì§€ê°€ ì •ì˜ëœ ë…¸ë“œì™€ ë¼ìš°í„°ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ëª¨ë“  ë…¸ë“œëŠ” ì‘ì—… ì™„ë£Œ í›„ ì¢…ë£Œ(END)ë¡œ ì—°ê²°
nodes_with_special_outgoing_edges = special_edge_sources.union({"router"})
for node_name in all_nodes:
    if node_name not in nodes_with_special_outgoing_edges:
        graph.add_edge(node_name, END)

agent = graph.compile(interrupt_after=["human_approval"])

# 7) FastAPI ì•± ìƒì„± ë° ì—”ë“œí¬ì¸íŠ¸
app = FastAPI()

@app.get("/v1/models")
async def list_models():
    """
    models.jsonì— ì •ì˜ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    OpenAIì˜ /v1/models ì—”ë“œí¬ì¸íŠ¸ì™€ í˜¸í™˜ë˜ëŠ” í˜•ì‹ì…ë‹ˆë‹¤.
    """
    all_models = model_registry.get_models()
    model_data = [
        {"id": model.model_id, "object": "model", "created": 1686935002, "owned_by": model.provider}
        for model in all_models
    ]
    return JSONResponse(content={
        "object": "list",
        "data": model_data
    })

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    # Pydantic ëª¨ë¸ì„ ë‚´ë¶€ ìƒíƒœ(dict)ë¡œ ë³€í™˜
    state = {
        "messages": [msg.model_dump() for msg in req.messages],
    }
    # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
    result = await agent.ainvoke(state)
    return ChatResponse(messages=result["messages"])

if __name__ == "__main__":
    import uvicorn

    # .env íŒŒì¼ ë¡œë“œ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ í•„ìš”)
    load_dotenv()

    # í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ ê°œë°œ ëª¨ë“œì™€ í”„ë¡œë•ì…˜ ëª¨ë“œë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
    # APP_ENVê°€ 'development'ì¼ ë•Œë§Œ hot-reloadingì„ í™œì„±í™”í•©ë‹ˆë‹¤.
    is_development = os.getenv("APP_ENV") == "development"

    print(f"ğŸš€ Starting server in {'development (hot-reload enabled)' if is_development else 'production'} mode.")

    uvicorn.run(
        "main:app",
        host="0.0.0.0", port=8000, reload=is_development
    )
