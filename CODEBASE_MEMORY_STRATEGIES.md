# 🧠 에이전트의 코드베이스 이해 및 기억 전략 (v2)

이 문서는 AI 에이전트가 대규모 코드베이스를 효과적으로 이해하고, 이를 '기억'하여 유지보수 및 개발 지원에 활용하기 위한 구체적인 설계와 전략을 기술합니다.

**핵심 원칙**:
1.  **사용자 투명성**: 이 모든 기억/정리 과정은 백그라운드에서 자동으로 일어나며, 사용자는 이를 전혀 인지하지 못합니다. 에이전트가 자연스럽게 '똑똑해지는' 경험을 제공하는 것을 목표로 합니다.
2.  **기존 로직과의 호환성**: '서기 에이전트'와 '기억 압축 서비스'는 기존의 RAG 검색 로직이나 다른 도구 실행에 영향을 주지 않는 독립적인 모듈로 작동합니다.

---

## 1. 아키텍처: '서기 에이전트'를 통한 동적 지식 축적 (Hybrid RAG)

현재 프로젝트는 정적 문서화와 동적 학습을 결합한 하이브리드 RAG 접근 방식을 채택합니다. 핵심은 **'서기(Scribe) 에이전트'** 라는 개념을 도입하여, 대화 속에서 자연스럽게 발생하는 설계 결정과 규칙을 자동으로 포착하고 축적하는 것입니다.

```
+-----------------------+      +-------------------------+      
|    Main Agent (API)   |<---->|   User (e.g., via UI)   |      
+-----------------------+      +-------------------------+      
           | 1. 대화 발생                                             
           |                                                          
           v                                                        
+-----------------------+      5. RAG 검색 요청 (향상된 컨텍스트)
| Tool Dispatcher & LLM |<------------------------------------------+
+-----------------------+                                           |
           | 2. 대화 내용 전송 (Async)                                |
           v                                                        |
+-----------------------+      3. 지식 추출 및 구조화                  |
|  Scribe Agent (Async) |----------------->+                         |
+-----------------------+                    |                         |
                                             v                         |
+--------------------------------------------------------------------+  
|                            Knowledge Store                           |  
| +------------------------+  +--------------------------------------+ |  
| | Vector DB (ChromaDB)   |  |  Relational DB (MariaDB - 'memories')| |  
| | - content_embedding    |  |  - id (PK), type, content, source    | |  
| +------------------------+  |  - metadata (JSON), created_at       | |  
|                             |  - last_accessed_at, access_count    | |  
|                             +--------------------------------------+ |  
+--------------------------------------------------------------------+  
           ^                                                        
           | 4. 주기적 정리/요약                                      
           |                                                        
+--------------------------+                                        
| Memory Compaction Service|                                        
| (Scheduled Task)         |                                        
+--------------------------+                                        
```

### 1.1. 서기 에이전트 (Scribe Agent)

-   **역할**: 메인 에이전트의 대화를 비동기적으로 분석하여 영구적으로 저장할 가치가 있는 '기억 조각(Memory Chunk)'을 추출하고 구조화합니다.
-   **실행 시점**: 메인 에이전트의 응답이 완료된 직후, 비동기(background) 태스크로 실행되어 사용자 응답 시간에 영향을 주지 않습니다.
-   **지식 추출 로직 (LLM 기반)**:
    1.  최근 N개의 대화 턴(예: 최근 5턴)을 가져옵니다.
    2.  강력한 LLM(예: GPT-4o, Claude 3 Opus)에게 아래와 같은 시스템 프롬프트를 사용하여 지식 추출을 요청합니다.

    > **[시스템 프롬프트 예시]**
    >
    > 당신은 대화의 핵심을 파악하는 뛰어난 아키텍트입니다. 주어진 대화 내용에서 아래 4가지 유형에 해당하는, 장기적으로 기억할 가치가 있는 정보만 추출하여 지정된 JSON 형식으로 출력하세요. 정보가 없다면 빈 배열 `[]`을 반환하세요.
    >
    > 1.  **Decision**: 명시적인 기술적/설계적 결정 (예: "DB는 MariaDB를 사용하기로 했습니다.")
    > 2.  **Rule**: 반드시 지켜야 할 규칙이나 제약 조건 (예: "모든 API는 snake_case로 작성해야 합니다.")
    > 3.  **Fact**: 나중에 참고할 만한 객관적인 사실이나 정보 (예: "A 모듈의 담당자는 김철수입니다.")
    > 4.  **OpenQuestion**: 아직 해결되지 않았지만 나중에 논의해야 할 중요한 질문 (예: "인증 방식을 JWT로 할지, OAuth로 할지 고민되네요.")
    >
    > **출력 JSON 형식:**
    >
    > ```json
    > [
    >   {
    >     "type": "Decision",
    >     "content": "DB는 MariaDB를 사용하기로 결정함.",
    >     "importance": "high",
    >     "related_files": ["docker-compose.yml", "core/database.py"]
    >   },
    >   {
    >     "type": "Rule",
    >     "content": "모든 API의 경로는 소문자와 하이픈(-)만 사용해야 함.",
    >     "importance": "medium",
    >     "related_files": ["api/router.py"]
    >   }
    > ]
    > ```

-   **처리 결과**: 추출된 JSON 배열을 순회하며 각 '기억 조각'을 **Knowledge Store**에 저장합니다.

### 1.2. 지식 저장소 (Knowledge Store)

-   **역할**: 서기 에이전트가 추출한 '기억 조각'을 영구적으로 저장하고, RAG 검색을 위해 인덱싱합니다.
-   **기술 스택**:
    -   **MariaDB (또는 다른 RDBMS)**: 메타데이터와 원본 콘텐츠를 구조적으로 저장합니다.
    -   **rag-pipeline 서버 연동**: `content`를 임베딩하고 벡터 검색을 수행하는 모든 작업은 `rag-pipeline` 서버를 통해 이루어집니다. 백엔드는 직접 벡터 DB를 제어하지 않습니다.
-   **`memories` 테이블 스키마 (MariaDB)**:
    | 필드명 | 타입 | 설명 |
    | :--- | :--- | :--- |
    | `id` | `VARCHAR(36)` | UUID, Primary Key |
    | `type` | `VARCHAR(20)` | 기억 유형 (Decision, Rule, Fact, OpenQuestion, Summary) |
    | `content` | `TEXT` | 기억의 실제 내용 |
    | `metadata` | `JSON` | 출처, 관련 파일, 중요도 등 추가 정보 |
    | `source_session_id` | `VARCHAR(36)` | 이 기억이 발생한 채팅 세션 ID |
    | `access_count` | `INT` | 이 기억이 RAG 검색에서 사용된 횟수 |
    | `last_accessed_at` | `DATETIME` | 마지막으로 사용된 시간 |
    | `created_at` | `DATETIME` | 생성 시간 |
    | `is_archived` | `BOOLEAN` | 보관 처리 여부 |

### 1.3. 기억 압축 서비스 (Memory Compaction Service)

-   **역할**: 지식 저장소가 비대해지는 것을 막고 최신성을 유지하기 위해, 주기적으로 기억을 정리하고 요약합니다.
-   **실행 시점**: 스케줄러(예: `APScheduler`)를 통해 매일 자정 등 정해진 시간에 실행됩니다.
-   **주요 로직**:
    1.  **오래된 정보 식별**: `last_accessed_at`이 오래되었거나(예: 3개월 이상), `access_count`가 매우 낮은 기억들을 찾습니다.
    2.  **요약 및 병합**:
        -   특정 `related_files`나 주제와 관련된 기억 조각이 여러 개 쌓이면(예: 10개 이상), LLM을 호출하여 해당 조각들을 하나의 요약된 'Summary' 타입의 기억으로 병합합니다.
        -   병합된 기존 기억 조각들은 `is_archived = true`로 설정하여 비활성화합니다.
    3.  **완전 삭제**: `type`이 `OpenQuestion`인데, 관련된 새로운 `Decision`이 생긴 경우 등 명확하게 해결된 정보는 삭제할 수 있습니다.

### 1.4. RAG 통합

-   **역할**: 메인 에이전트가 사용자의 질문에 답변할 때, 일반적인 코드베이스 정보뿐만 아니라 **Knowledge Store**의 정제된 기억을 함께 참고하도록 합니다.
-   **검색 로직**:
    1.  사용자 질문이 들어오면, `rag-pipeline` 서버에 요청하여 **Knowledge Store**에서 의미적으로 가장 유사한 기억 조각 N개를 검색합니다.
    2.  그 다음, `rag-pipeline` 서버를 통해 일반 코드베이스 RAG에서 관련 코드 스니펫 M개를 검색합니다.
    3.  두 검색 결과를 합치고, 중요도(`importance`)나 최근성(`created_at`)에 따라 순위를 재조정하여 최종 컨텍스트를 구성합니다.
-   **프롬프트 구성**: 최종적으로 구성된 컨텍스트를 LLM 프롬프트에 아래와 같이 포함시킵니다.

    > **[프롬프트 예시]**
    >
    > ... (기본 시스템 프롬프트) ...
    >
    > **[최근 대화에서 축적된 관련 기억]**
    >
    > -   **규칙**: 모든 API 경로는 소문자와 하이픈만 사용해야 함 (출처: chat_session_abc)
    > -   **결정**: DB는 MariaDB를 사용하기로 함 (출처: chat_session_def)
    >
    > **[관련 코드 스니펫]**
    >
    > ```python
    > # /api/router.py
    > ... (코드 내용) ...
    > ```
    >
    > **[사용자 질문]**
    >
    > "새로운 유저 API를 만들어줘"

---

## 2. 기타 고려사항: 코드 지식 그래프 (Code Knowledge Graph, CKG)

궁극적으로 코드의 깊은 설계 이해와 정확한 영향도 분석을 위해서는 **코드 지식 그래프(CKG)와의 통합**이 다음 단계의 목표가 될 수 있습니다. LLM은 이 과정에서 그래프를 구축하고 쿼리하는 데 핵심적인 역할을 수행할 수 있습니다.

-   **RAG + CKG**: RAG를 통해 관련 코드 스니펫을 빠르게 검색하고, CKG를 통해 코드의 구조적/관계적 정보를 얻어 LLM의 이해를 심화시키는 가장 유망한 하이브리드 방식입니다.
